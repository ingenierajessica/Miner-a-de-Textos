{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINERÍA DE TEXTOS\n",
    "\n",
    "## Alumna: Jessica Sarahi Méndez Rincón\n",
    "\n",
    "\n",
    "###  Proyecto Análisis de Sentimientos y Polaridad en Tweets\n",
    "###  Tema: COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de librerias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#Tienes que descargarte las stopwords primero via nltk.download()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto:\n",
    "    \n",
    "    El año 2020 se vio afectado por el impacto a la sociedad mundial sobre una gripe de alto nivel de contagio así como la causa principal de fallecimientos alrededor del Mundo.\n",
    "    \n",
    "    México no ha sido exento del raro virus.\n",
    "    \n",
    "    Con el presente Proyecto, el objetivo primordial es desarrollar los algoritmos que ayuden a monitorizar el efecto en los comentarios que en la Red Social de Twitter se emiten, y que ayudé a dar una idea del estado de ánimo de la gente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQUpnfdhCKN1C3OIbNrDdyjctlRsA9t6386WWD8JkUzt_X7JiuH&usqp=CAU\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion del archivo que se nos proporciono agregandole cabeceras que no contiene\n",
    "names = ['Id', 'tweet', 'polaridad']\n",
    "dataset = pd.read_csv('train.txt', sep='\\t',names=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polaridad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>818942405281980417</td>\n",
       "      <td>No mames este pinche dolor que pedo? ya mejor ...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>818963123646373892</td>\n",
       "      <td>@leomall2018 Según yo era como aviso, pero aho...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>818998133694697472</td>\n",
       "      <td>@benshorts a juzgar por mis comportamientos au...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>818822556090105857</td>\n",
       "      <td>#BuenosDias mundo Twittero ya desperté y estoy...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>818993011514372098</td>\n",
       "      <td>No pude resolver el rompecabezas en Los rios d...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id                                              tweet  \\\n",
       "0  818942405281980417  No mames este pinche dolor que pedo? ya mejor ...   \n",
       "1  818963123646373892  @leomall2018 Según yo era como aviso, pero aho...   \n",
       "2  818998133694697472  @benshorts a juzgar por mis comportamientos au...   \n",
       "3  818822556090105857  #BuenosDias mundo Twittero ya desperté y estoy...   \n",
       "4  818993011514372098  No pude resolver el rompecabezas en Los rios d...   \n",
       "\n",
       "  polaridad  \n",
       "0         N  \n",
       "1       NEU  \n",
       "2         N  \n",
       "3         P  \n",
       "4         N  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      No mames este pinche dolor que pedo? ya mejor ...\n",
       "1      @leomall2018 Según yo era como aviso, pero aho...\n",
       "2      @benshorts a juzgar por mis comportamientos au...\n",
       "3      #BuenosDias mundo Twittero ya desperté y estoy...\n",
       "4      No pude resolver el rompecabezas en Los rios d...\n",
       "                             ...                        \n",
       "985    @ladelbosque29 acude al próximo llamado que ha...\n",
       "986    @Dianybony jajajaja claro que no amor!! te amo...\n",
       "987    Hoy le pedí a Dios una señal realmente obvia, ...\n",
       "988    El reboot de Jumanji puede romper mi corazón x...\n",
       "989    @Djrossana que tengan un lindo martes y que to...\n",
       "Name: tweet, Length: 990, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'NEU' 'N' 'P' 'N' 'N' 'NEU' 'N' 'N' 'P' 'N' 'P' 'P' 'NEU' 'N' 'N' 'P'\n",
      " 'N' 'N' 'N' 'N' 'NEU' 'N' 'N' 'P' 'N' 'N' 'N' 'NEU' 'N' 'NEU' 'NEU' 'P'\n",
      " 'N' 'N' 'P' 'P' 'NEU' 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N' 'N' 'P'\n",
      " 'P' 'N' 'N' 'N' 'P' 'N' 'P' 'P' 'N' 'P' 'P' 'P' 'N' 'P' 'N' 'N' 'N' 'N'\n",
      " 'P' 'P' 'P' 'N' 'NEU' 'P' 'NEU' 'N' 'NEU' 'NEU' 'P' 'N' 'N' 'P' 'NEU' 'N'\n",
      " 'N' 'N' 'N' 'P' 'N' 'NEU' 'N' 'N' 'N' 'P' 'NEU' 'N' 'N' 'N' 'N' 'NEU' 'P'\n",
      " 'NEU' 'P' 'N' 'P' 'NEU' 'NEU' 'N' 'N' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N'\n",
      " 'P' 'P' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'NEU' 'NEU' 'N' 'NEU' 'P' 'P' 'NEU'\n",
      " 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'N' 'N' 'NEU' 'NEU' 'P' 'NEU' 'P' 'N' 'N' 'P'\n",
      " 'P' 'N' 'P' 'P' 'P' 'N' 'N' 'NEU' 'P' 'NEU' 'P' 'P' 'P' 'P' 'N' 'NEU'\n",
      " 'NEU' 'P' 'N' 'N' 'NEU' 'N' 'P' 'N' 'P' 'N' 'P' 'NEU' 'NEU' 'P' 'N' 'N'\n",
      " 'N' 'P' 'P' 'NEU' 'N' 'P' 'N' 'N' 'P' 'N' 'N' 'P' 'N' 'NEU' 'N' 'N' 'N'\n",
      " 'NEU' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'NEU' 'NEU' 'NEU' 'P' 'NEU' 'NEU'\n",
      " 'N' 'N' 'N' 'P' 'N' 'NEU' 'P' 'P' 'N' 'P' 'N' 'N' 'NEU' 'P' 'P' 'NEU' 'N'\n",
      " 'P' 'N' 'P' 'P' 'N' 'P' 'P' 'NEU' 'N' 'N' 'N' 'NEU' 'P' 'NEU' 'P' 'NEU'\n",
      " 'N' 'N' 'N' 'P' 'N' 'NEU' 'N' 'P' 'P' 'N' 'N' 'NEU' 'N' 'N' 'N' 'P' 'N'\n",
      " 'N' 'N' 'N' 'NEU' 'N' 'N' 'NEU' 'N' 'P' 'N' 'P' 'N' 'N' 'NEU' 'N' 'N' 'P'\n",
      " 'P' 'P' 'N' 'P' 'NEU' 'N' 'N' 'N' 'N' 'NEU' 'N' 'N' 'N' 'P' 'N' 'NEU' 'N'\n",
      " 'N' 'N' 'N' 'N' 'NEU' 'P' 'N' 'NEU' 'N' 'P' 'NEU' 'P' 'NEU' 'P' 'N' 'N'\n",
      " 'N' 'P' 'N' 'NEU' 'N' 'NEU' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'N' 'NEU' 'N' 'P'\n",
      " 'N' 'P' 'N' 'P' 'N' 'N' 'N' 'P' 'N' 'NEU' 'N' 'N' 'N' 'N' 'NEU' 'P' 'N'\n",
      " 'NEU' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'NEU' 'NEU' 'P' 'N' 'N' 'P' 'P' 'N' 'P'\n",
      " 'N' 'N' 'P' 'P' 'N' 'P' 'P' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'N' 'N' 'P'\n",
      " 'N' 'N' 'N' 'P' 'P' 'NEU' 'NEU' 'N' 'N' 'N' 'NEU' 'NEU' 'P' 'P' 'NEU' 'N'\n",
      " 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'N' 'NEU' 'N' 'N'\n",
      " 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'NEU' 'NEU'\n",
      " 'N' 'P' 'P' 'NEU' 'NEU' 'N' 'N' 'N' 'NEU' 'P' 'N' 'N' 'P' 'N' 'N' 'P' 'P'\n",
      " 'P' 'NEU' 'NEU' 'N' 'NEU' 'N' 'P' 'N' 'N' 'P' 'N' 'P' 'N' 'N' 'N' 'NEU'\n",
      " 'P' 'P' 'N' 'N' 'N' 'N' 'NEU' 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'NEU' 'N' 'P'\n",
      " 'N' 'N' 'N' 'N' 'N' 'NEU' 'N' 'N' 'N' 'NEU' 'N' 'N' 'N' 'P' 'N' 'N' 'N'\n",
      " 'P' 'NEU' 'N' 'P' 'N' 'NEU' 'NEU' 'P' 'NEU' 'N' 'N' 'NEU' 'P' 'NEU' 'N'\n",
      " 'N' 'P' 'NEU' 'NEU' 'NEU' 'N' 'P' 'NEU' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'P'\n",
      " 'P' 'N' 'P' 'N' 'P' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'N' 'NEU' 'P'\n",
      " 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'NEU' 'P' 'N' 'N' 'N' 'NEU' 'N' 'NEU' 'N' 'N'\n",
      " 'NEU' 'NEU' 'P' 'P' 'N' 'N' 'NEU' 'N' 'N' 'N' 'P' 'P' 'N' 'P' 'N' 'P' 'P'\n",
      " 'N' 'P' 'P' 'N' 'P' 'N' 'N' 'P' 'NEU' 'NEU' 'NEU' 'NEU' 'P' 'NEU' 'N'\n",
      " 'NEU' 'P' 'P' 'P' 'NEU' 'P' 'P' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'N' 'N'\n",
      " 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'P' 'NEU' 'P' 'N' 'N' 'NEU' 'N' 'P' 'P'\n",
      " 'P' 'P' 'N' 'P' 'P' 'N' 'P' 'N' 'P' 'NEU' 'NEU' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'NEU' 'P' 'P' 'P' 'N' 'N' 'P' 'NEU' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'N'\n",
      " 'NEU' 'N' 'N' 'NEU' 'P' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'NEU' 'N' 'P' 'P' 'P'\n",
      " 'N' 'NEU' 'P' 'P' 'N' 'NEU' 'N' 'P' 'NEU' 'P' 'N' 'NEU' 'N' 'N' 'N' 'NEU'\n",
      " 'NEU' 'P' 'P' 'N' 'P' 'P' 'P' 'N' 'N' 'P' 'NEU' 'N' 'P' 'P' 'N' 'N' 'P'\n",
      " 'N' 'N' 'NEU' 'N' 'NEU' 'N' 'NEU' 'N' 'NEU' 'P' 'P' 'N' 'N' 'P' 'N' 'P'\n",
      " 'N' 'N' 'N' 'NEU' 'N' 'P' 'N' 'NEU' 'N' 'N' 'P' 'P' 'P' 'N' 'NEU' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'NEU' 'N' 'N' 'N' 'NEU'\n",
      " 'N' 'NEU' 'N' 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'P' 'N' 'P' 'P' 'N'\n",
      " 'NEU' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'N'\n",
      " 'P' 'N' 'P' 'NEU' 'N' 'P' 'NEU' 'P' 'NEU' 'P' 'P' 'N' 'N' 'N' 'N' 'P' 'P'\n",
      " 'NEU' 'N' 'P' 'N' 'N' 'NEU' 'P' 'N' 'N' 'NEU' 'P' 'NEU' 'P' 'N' 'P' 'NEU'\n",
      " 'P' 'N' 'NEU' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'NEU' 'N' 'P' 'N' 'N' 'P'\n",
      " 'NEU' 'N' 'P' 'NEU' 'N' 'N' 'N' 'NEU' 'P' 'N' 'N' 'P' 'N' 'N' 'P' 'P' 'N'\n",
      " 'N' 'N' 'P' 'P' 'N' 'NEU' 'P' 'P' 'P' 'P' 'P' 'P' 'NEU' 'N' 'P' 'N' 'P'\n",
      " 'P' 'N' 'P' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'P' 'NEU' 'NEU' 'P' 'N' 'P'\n",
      " 'NEU' 'N' 'P' 'P' 'N' 'N' 'N' 'P' 'NEU' 'N' 'N' 'N' 'N' 'N' 'P' 'NEU' 'P'\n",
      " 'NEU' 'NEU' 'P' 'N' 'N' 'N' 'NEU' 'P' 'NEU' 'P' 'P' 'P' 'P' 'N' 'P' 'NEU'\n",
      " 'N' 'N' 'P' 'N' 'N' 'N' 'NEU' 'N' 'P' 'N' 'N' 'N' 'P' 'N' 'NEU' 'NEU' 'N'\n",
      " 'N' 'N' 'P' 'NEU' 'NEU' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'P' 'P' 'P' 'N' 'NEU'\n",
      " 'P' 'P' 'P' 'N' 'N' 'N' 'NEU' 'N' 'N' 'N' 'N' 'NEU' 'N' 'NEU' 'P' 'P' 'N'\n",
      " 'P']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "Train_X = dataset.iloc[:, 1].values\n",
    "Train_Y = dataset.iloc[:, 2].values\n",
    "print(Train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_stopwords = stopwords.words('spanish')\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "non_words = list(punctuation)\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str,range(10)))\n",
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    tokens =  word_tokenize(text)\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#Definimos el vectorizer de nuevo y creamos un pipeline de vectorizer -> classificador\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC() es el clasificador\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "#('vect', CountVectorizer()),\n",
    "#('tfidf', TfidfTransformer()),\n",
    "#('clf', SGDClassifier()),   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui definimos el espacio de parámetros a explorar\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigramas or bigramas\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__loss': ('hinge', 'squared_hinge'),\n",
    "    'cls__max_iter': (500, 1000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 , scoring='roc_auc')\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "Encoder = LabelEncoder() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['polaridadF']=Encoder.fit_transform(dataset.polaridad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polaridad</th>\n",
       "      <th>polaridadF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>818942405281980417</td>\n",
       "      <td>No mames este pinche dolor que pedo? ya mejor ...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>818963123646373892</td>\n",
       "      <td>@leomall2018 Según yo era como aviso, pero aho...</td>\n",
       "      <td>NEU</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>818998133694697472</td>\n",
       "      <td>@benshorts a juzgar por mis comportamientos au...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>818822556090105857</td>\n",
       "      <td>#BuenosDias mundo Twittero ya desperté y estoy...</td>\n",
       "      <td>P</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>818993011514372098</td>\n",
       "      <td>No pude resolver el rompecabezas en Los rios d...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id                                              tweet  \\\n",
       "0  818942405281980417  No mames este pinche dolor que pedo? ya mejor ...   \n",
       "1  818963123646373892  @leomall2018 Según yo era como aviso, pero aho...   \n",
       "2  818998133694697472  @benshorts a juzgar por mis comportamientos au...   \n",
       "3  818822556090105857  #BuenosDias mundo Twittero ya desperté y estoy...   \n",
       "4  818993011514372098  No pude resolver el rompecabezas en Los rios d...   \n",
       "\n",
       "  polaridad  polaridadF  \n",
       "0         N           0  \n",
       "1       NEU           1  \n",
       "2         N           0  \n",
       "3         P           2  \n",
       "4         N           0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando parametros ideales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=['de', 'la',\n",
       "                                                                    'que', 'el',\n",
       "                                                                    'en', 'y',\n",
       "                                                                    'a', 'los'...\n",
       "                                                  verbose=0))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'cls__C': (0.2, 0.5, 0.7),\n",
       "                         'cls__loss': ('hinge', 'squared_hinge'),\n",
       "                         'cls__max_iter': (500, 1000),\n",
       "                         'vect__max_df': (0.5, 1.9),\n",
       "                         'vect__max_features': (500, 1000),\n",
       "                         'vect__min_df': (10, 20, 50),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(dataset.tweet, dataset.polaridadF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor parámetro es:\n",
      "{'cls__C': 0.2, 'cls__loss': 'hinge', 'cls__max_iter': 500, 'vect__max_df': 0.5, 'vect__max_features': 500, 'vect__min_df': 10, 'vect__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "print(\"El mejor parámetro es:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=.2, loss='hinge', max_iter=1000,\n",
    "multi_class='ovr',random_state=None, penalty='l2',tol=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    " analyzer = 'word',\n",
    " tokenizer = tokenize,\n",
    " lowercase = True,\n",
    " stop_words = spanish_stopwords,\n",
    " min_df = 10,\n",
    " max_df = 1.9,\n",
    " ngram_range=(1, 1),\n",
    " max_features=1000)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de éxito:\n",
      "0.6181818181818182\n"
     ]
    }
   ],
   "source": [
    "corpus_data_features = vectorizer.fit_transform(dataset.tweet)\n",
    "corpus_data_features_nd = corpus_data_features.toarray()\n",
    "scores = cross_val_score(\n",
    " model,\n",
    " corpus_data_features_nd[0:len(dataset)],\n",
    " y=dataset.polaridad, \n",
    " cv=5\n",
    " )\n",
    "print(\"Probabilidad de éxito:\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.9,\n",
       "                                 max_features=1000, min_df=10,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=['de', 'la', 'que', 'el', 'en', 'y',\n",
       "                                             'a', 'los', 'del', 'se', 'las',\n",
       "                                             'por', 'un', 'para', 'con', 'no'...\n",
       "                                             'o', 'este', 'sí', 'porque', ...],\n",
       "                                 strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at 0x0000019CA00D3558>,\n",
       "                                 vocabulary=None)),\n",
       "                ('cls',\n",
       "                 LinearSVC(C=0.2, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    " ('vect', CountVectorizer(\n",
    " analyzer='word',\n",
    " tokenizer=tokenize,\n",
    " lowercase=True,\n",
    " stop_words=spanish_stopwords,\n",
    " min_df=10,\n",
    " max_df=1.9,\n",
    " ngram_range=(1, 1),\n",
    " max_features=1000\n",
    " )),\n",
    " ('cls', LinearSVC(\n",
    " C=.2,\n",
    " loss='squared_hinge',\n",
    " max_iter=1000,\n",
    " multi_class='ovr',\n",
    " random_state=None,\n",
    " penalty='l2',\n",
    " tol=0.0001\n",
    " )),\n",
    "])\n",
    "pipeline.fit(dataset.tweet, dataset.polaridad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-04d83ab31cac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'polarity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweets['polarity'] = pipeline.predict(tweets.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
