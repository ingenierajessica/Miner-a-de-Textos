{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 12 - Análisis de sentimientos\n",
    "\n",
    "# MINERÍA DE TEXTOS\n",
    "\n",
    "## Alumna: Jessica Sarahi Méndez Rincón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Descarga de corpus Movie reviews: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "2 - Usar el promedio de palabras en cada review para representar el documento.\n",
    "\n",
    "3 - Usar doc2vec para obtener el vector del documento.\n",
    "\n",
    "4 - Elegir uno de los otros métodos vistos para generar el vector de documento.\n",
    "\n",
    "5 - Verificar que método obtiene mejor exactitud (accuracy) para clasificar los reviews en las etiquetas de sentimiento correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    " \n",
    "# leer el archivo dentro de la memoria\n",
    "def load_doc(filename):\n",
    "    # abrir el archivo y leerlo únicamente\n",
    "    file = open(filename, 'r')\n",
    "    #leer todo el texto\n",
    "    text = file.read()\n",
    "    # cerrar el archivo\n",
    "    file.close()\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0_10.txt\n",
      "Loaded 10000_7.txt\n",
      "Loaded 10001_9.txt\n",
      "Loaded 10002_8.txt\n",
      "Loaded 10003_8.txt\n",
      "Loaded 10004_9.txt\n",
      "Loaded 10005_8.txt\n",
      "Loaded 10006_7.txt\n",
      "Loaded 10007_10.txt\n",
      "Loaded 10008_8.txt\n",
      "Loaded 10009_10.txt\n",
      "Loaded 1000_9.txt\n",
      "Loaded 10010_9.txt\n",
      "Loaded 10011_9.txt\n",
      "Loaded 10012_9.txt\n",
      "Loaded 10013_9.txt\n",
      "Loaded 10014_7.txt\n",
      "Loaded 10015_8.txt\n",
      "Loaded 10016_8.txt\n",
      "Loaded 10017_8.txt\n",
      "Loaded 10018_8.txt\n",
      "Loaded 10019_8.txt\n",
      "Loaded 1001_10.txt\n",
      "Loaded 10020_8.txt\n",
      "Loaded 10021_9.txt\n",
      "Loaded 10022_10.txt\n",
      "Loaded 10023_8.txt\n",
      "Loaded 10024_9.txt\n",
      "Loaded 10025_8.txt\n",
      "Loaded 10026_10.txt\n",
      "Loaded 10027_8.txt\n",
      "Loaded 10028_10.txt\n",
      "Loaded 10029_10.txt\n",
      "Loaded 1002_9.txt\n",
      "Loaded 10030_7.txt\n",
      "Loaded 10031_8.txt\n",
      "Loaded 10032_8.txt\n",
      "Loaded 10033_8.txt\n",
      "Loaded 10034_9.txt\n",
      "Loaded 10035_10.txt\n",
      "Loaded 10036_10.txt\n",
      "Loaded 10037_7.txt\n",
      "Loaded 10038_7.txt\n",
      "Loaded 10039_8.txt\n",
      "Loaded 1003_8.txt\n",
      "Loaded 10040_10.txt\n",
      "Loaded 10041_8.txt\n",
      "Loaded 10042_8.txt\n",
      "Loaded 10043_10.txt\n",
      "Loaded 10044_8.txt\n",
      "Loaded 10045_7.txt\n",
      "Loaded 10046_10.txt\n",
      "Loaded 10047_10.txt\n",
      "Loaded 10048_8.txt\n",
      "Loaded 10049_8.txt\n",
      "Loaded 1004_10.txt\n",
      "Loaded 10050_8.txt\n",
      "Loaded 10051_8.txt\n",
      "Loaded 10052_7.txt\n",
      "Loaded 10053_10.txt\n",
      "Loaded 10054_7.txt\n",
      "Loaded 10055_10.txt\n",
      "Loaded 10056_8.txt\n",
      "Loaded 10057_8.txt\n",
      "Loaded 10058_8.txt\n",
      "Loaded 10059_10.txt\n",
      "Loaded 1005_7.txt\n",
      "Loaded 10060_10.txt\n",
      "Loaded 10061_10.txt\n",
      "Loaded 10062_9.txt\n",
      "Loaded 10063_10.txt\n",
      "Loaded 10064_10.txt\n",
      "Loaded 10065_8.txt\n",
      "Loaded 10066_7.txt\n",
      "Loaded 10067_7.txt\n",
      "Loaded 10068_8.txt\n",
      "Loaded 10069_10.txt\n",
      "Loaded 1006_8.txt\n",
      "Loaded 10070_10.txt\n",
      "Loaded 10071_8.txt\n",
      "Loaded 10072_10.txt\n",
      "Loaded 10073_7.txt\n",
      "Loaded 10074_7.txt\n",
      "Loaded 10075_10.txt\n",
      "Loaded 10076_10.txt\n",
      "Loaded 10077_7.txt\n",
      "Loaded 10078_10.txt\n",
      "Loaded 10079_10.txt\n",
      "Loaded 1007_9.txt\n",
      "Loaded 10080_10.txt\n",
      "Loaded 10081_10.txt\n",
      "Loaded 10082_8.txt\n",
      "Loaded 10083_10.txt\n",
      "Loaded 10084_10.txt\n",
      "Loaded 10085_10.txt\n",
      "Loaded 10086_10.txt\n",
      "Loaded 10087_7.txt\n",
      "Loaded 10088_8.txt\n",
      "Loaded 10089_9.txt\n",
      "Loaded 1008_10.txt\n",
      "Loaded 10090_10.txt\n",
      "Loaded 10091_10.txt\n",
      "Loaded 10092_8.txt\n",
      "Loaded 10093_10.txt\n",
      "Loaded 10094_9.txt\n",
      "Loaded 10095_9.txt\n",
      "Loaded 10096_9.txt\n",
      "Loaded 10097_10.txt\n",
      "Loaded 10098_9.txt\n",
      "Loaded 10099_9.txt\n",
      "Loaded 1009_8.txt\n",
      "Loaded 100_10.txt\n",
      "Loaded 10100_10.txt\n",
      "Loaded 10101_9.txt\n",
      "Loaded 10102_8.txt\n",
      "Loaded 10103_9.txt\n",
      "Loaded 10104_10.txt\n",
      "Loaded 10105_8.txt\n",
      "Loaded 10106_10.txt\n",
      "Loaded 10107_9.txt\n",
      "Loaded 10108_8.txt\n",
      "Loaded 10109_10.txt\n",
      "Loaded 1010_9.txt\n",
      "Loaded 10110_10.txt\n",
      "Loaded 10111_10.txt\n",
      "Loaded 10112_10.txt\n",
      "Loaded 10113_9.txt\n",
      "Loaded 10114_8.txt\n",
      "Loaded 10115_8.txt\n",
      "Loaded 10116_10.txt\n",
      "Loaded 10117_9.txt\n",
      "Loaded 10118_8.txt\n",
      "Loaded 10119_10.txt\n",
      "Loaded 1011_8.txt\n",
      "Loaded 10120_10.txt\n",
      "Loaded 10121_10.txt\n",
      "Loaded 10122_10.txt\n",
      "Loaded 10123_10.txt\n",
      "Loaded 10124_8.txt\n",
      "Loaded 10125_8.txt\n",
      "Loaded 10126_9.txt\n",
      "Loaded 10127_10.txt\n",
      "Loaded 10128_10.txt\n",
      "Loaded 10129_10.txt\n",
      "Loaded 1012_8.txt\n",
      "Loaded 10130_8.txt\n",
      "Loaded 10131_7.txt\n",
      "Loaded 10132_10.txt\n",
      "Loaded 10133_8.txt\n",
      "Loaded 10134_7.txt\n",
      "Loaded 10135_8.txt\n",
      "Loaded 10136_10.txt\n",
      "Loaded 10137_10.txt\n",
      "Loaded 10138_10.txt\n",
      "Loaded 10139_10.txt\n",
      "Loaded 1013_7.txt\n",
      "Loaded 10140_10.txt\n",
      "Loaded 10141_8.txt\n",
      "Loaded 10142_10.txt\n",
      "Loaded 10143_9.txt\n",
      "Loaded 10144_8.txt\n",
      "Loaded 10145_8.txt\n",
      "Loaded 10146_10.txt\n",
      "Loaded 10147_7.txt\n",
      "Loaded 10148_10.txt\n",
      "Loaded 10149_9.txt\n",
      "Loaded 1014_8.txt\n",
      "Loaded 10150_8.txt\n",
      "Loaded 10151_8.txt\n",
      "Loaded 10152_9.txt\n",
      "Loaded 10153_10.txt\n",
      "Loaded 10154_10.txt\n",
      "Loaded 10155_7.txt\n",
      "Loaded 10156_8.txt\n",
      "Loaded 10157_8.txt\n",
      "Loaded 10158_9.txt\n",
      "Loaded 10159_7.txt\n",
      "Loaded 1015_8.txt\n",
      "Loaded 10160_7.txt\n",
      "Loaded 10161_7.txt\n",
      "Loaded 10162_10.txt\n",
      "Loaded 10163_7.txt\n",
      "Loaded 10164_9.txt\n",
      "Loaded 10165_9.txt\n",
      "Loaded 10166_8.txt\n",
      "Loaded 10167_9.txt\n",
      "Loaded 10168_9.txt\n",
      "Loaded 10169_8.txt\n",
      "Loaded 1016_7.txt\n",
      "Loaded 10170_8.txt\n",
      "Loaded 10171_8.txt\n",
      "Loaded 10172_9.txt\n",
      "Loaded 10173_8.txt\n",
      "Loaded 10174_9.txt\n",
      "Loaded 10175_7.txt\n",
      "Loaded 10176_9.txt\n",
      "Loaded 10177_10.txt\n",
      "Loaded 10178_10.txt\n",
      "Loaded 10179_10.txt\n",
      "Loaded 1017_7.txt\n",
      "Loaded 10180_8.txt\n",
      "Loaded 10181_10.txt\n",
      "Loaded 10182_10.txt\n",
      "Loaded 10183_9.txt\n",
      "Loaded 10184_9.txt\n",
      "Loaded 10185_10.txt\n",
      "Loaded 10186_10.txt\n",
      "Loaded 10187_10.txt\n",
      "Loaded 10188_10.txt\n",
      "Loaded 10189_10.txt\n",
      "Loaded 1018_7.txt\n",
      "Loaded 10190_10.txt\n",
      "Loaded 10191_10.txt\n",
      "Loaded 10192_10.txt\n",
      "Loaded 10193_7.txt\n",
      "Loaded 10194_7.txt\n",
      "Loaded 10195_10.txt\n",
      "Loaded 10196_8.txt\n",
      "Loaded 10197_8.txt\n",
      "Loaded 10198_8.txt\n",
      "Loaded 10199_10.txt\n",
      "Loaded 1019_10.txt\n",
      "Loaded 101_9.txt\n",
      "Loaded 10200_7.txt\n",
      "Loaded 10201_7.txt\n",
      "Loaded 10202_9.txt\n",
      "Loaded 10203_10.txt\n",
      "Loaded 10204_10.txt\n",
      "Loaded 10205_10.txt\n",
      "Loaded 10206_8.txt\n",
      "Loaded 10207_10.txt\n",
      "Loaded 10208_10.txt\n",
      "Loaded 10209_7.txt\n",
      "Loaded 1020_7.txt\n",
      "Loaded 10210_7.txt\n",
      "Loaded 10211_8.txt\n",
      "Loaded 10212_7.txt\n",
      "Loaded 10213_7.txt\n",
      "Loaded 10214_10.txt\n",
      "Loaded 10215_10.txt\n",
      "Loaded 10216_10.txt\n",
      "Loaded 10217_7.txt\n",
      "Loaded 10218_7.txt\n",
      "Loaded 10219_9.txt\n",
      "Loaded 1021_7.txt\n",
      "Loaded 10220_10.txt\n",
      "Loaded 10221_8.txt\n",
      "Loaded 10222_8.txt\n",
      "Loaded 10223_10.txt\n",
      "Loaded 10224_9.txt\n",
      "Loaded 10225_10.txt\n",
      "Loaded 10226_10.txt\n",
      "Loaded 10227_7.txt\n",
      "Loaded 10228_10.txt\n",
      "Loaded 10229_10.txt\n",
      "Loaded 1022_10.txt\n",
      "Loaded 10230_7.txt\n",
      "Loaded 10231_9.txt\n",
      "Loaded 10232_10.txt\n",
      "Loaded 10233_7.txt\n",
      "Loaded 10234_9.txt\n",
      "Loaded 10235_10.txt\n",
      "Loaded 10236_10.txt\n",
      "Loaded 10237_10.txt\n",
      "Loaded 10238_7.txt\n",
      "Loaded 10239_9.txt\n",
      "Loaded 1023_10.txt\n",
      "Loaded 10240_10.txt\n",
      "Loaded 10241_10.txt\n",
      "Loaded 10242_8.txt\n",
      "Loaded 10243_9.txt\n",
      "Loaded 10244_7.txt\n",
      "Loaded 10245_9.txt\n",
      "Loaded 10246_9.txt\n",
      "Loaded 10247_10.txt\n",
      "Loaded 10248_9.txt\n",
      "Loaded 10249_8.txt\n",
      "Loaded 1024_9.txt\n",
      "Loaded 10250_10.txt\n",
      "Loaded 10251_10.txt\n",
      "Loaded 10252_10.txt\n",
      "Loaded 10253_7.txt\n",
      "Loaded 10254_8.txt\n",
      "Loaded 10255_10.txt\n",
      "Loaded 10256_10.txt\n",
      "Loaded 10257_10.txt\n",
      "Loaded 10258_8.txt\n",
      "Loaded 10259_10.txt\n",
      "Loaded 1025_10.txt\n",
      "Loaded 10260_10.txt\n",
      "Loaded 10261_8.txt\n",
      "Loaded 10262_7.txt\n",
      "Loaded 10263_7.txt\n",
      "Loaded 10264_8.txt\n",
      "Loaded 10265_9.txt\n",
      "Loaded 10266_7.txt\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 1899: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c865f3121018>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#Especificar el directorio a leer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcarpeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test/pos'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprocess_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcarpeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-c865f3121018>\u001b[0m in \u001b[0;36mprocess_docs\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# leer el documento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loaded %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e4f79bb2b131>\u001b[0m in \u001b[0;36mload_doc\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#leer todo el texto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# cerrar el archivo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1899: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "#Leer todos los documentos en el directorio\n",
    "def process_docs(directory):\n",
    "    # recorrer todos los archivos de la carpeta\n",
    "    for filename in listdir(directory):\n",
    "        # omitir archivos que no tienen la extensión correcta\n",
    "        if not filename.endswith(\".txt\" ):\n",
    "            continue\n",
    "        # crear la ruta completa del archivo para abrir\n",
    "        path = directory + '/' + filename\n",
    "        # leer el documento\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    " \n",
    "#Especificar el directorio a leer\n",
    "carpeta = 'test/pos'\n",
    "process_docs(carpeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'this', 'movie!!', 'Sure', 'I', 'love', 'it', 'because', 'of', 'Madonna', 'but', 'who', 'cares', '-', \"it's\", 'damn', 'funny!!!', '*ALANiS', 'Rocks*.', 'When', 'I', 'first', 'saw', 'this', 'film', 'in', 'the', 'theatres', 'back', 'in', '1987,', 'I', 'thought', 'it', 'was', 'all', 'out', 'hilarious!', 'Madonna', 'is', 'so', 'funny', 'and', 'I', 'love', 'her', 'dubbed', 'accent', 'and', 'wacky/funky', 'look.', 'The', 'all-time', 'funniest', 'part', 'is', 'when', 'Madonna(Nikki)', 'screams', 'at', 'a', 'man', 'who', 'is', 'about', 'to', 'get', 'into', 'a', 'taxi.', 'And', 'also', 'when', 'Griffin', 'Dunne(Louden)trips', 'and', 'falls', 'at', 'the', 'apartment', 'interview', 'scene.', '**ALANiS', 'Rocks**.', \"Madonna's\", 'character', 'Nikki', 'steals/shop', 'lifts', 'and', 'fools', 'people', 'throughout', 'the', 'whole', 'movie', '-', 'her', 'hilarious', 'antics', 'are', 'enough', 'to', 'keep', 'you', 'on', 'the', 'floor', 'the', 'whole', 'time.', '\"Didn\\'t', 'rob', \"nothin',\", 'when', 'you', 'rob', 'a', 'store', 'you', 'stick', 'up', 'the', 'cashier.', 'We', 'busted', 'a', 'few', 'tapes,', \"there's\", 'a', 'bit', 'of', 'a', 'difference\"', 'I', 'love', 'that!!!', \"It's\", 'classic.', '***ALANiS', 'Rocks***.', 'I', \"don't\", 'know', 'why', 'this', 'movie', 'got', 'slammed', 'the', 'way', 'it', 'did.', 'I', 'see', 'nothing', 'wrong', 'with', 'it', '-', 'course', 'maybe', 'if', \"you're\", 'a', 'huge', 'Madonna', 'fan', 'then', 'whatever', 'she', 'does', 'is', 'just', 'awesome.', 'Anyone', 'out', 'there', 'who', 'wants', 'to', 'see', 'some', 'funny,', 'classic', 'entertainment', 'then', 'watch', '\"Who\\'s', 'That', 'Girl?\"', 'And', 'another', 'very', 'important', 'fact', 'that', 'of', 'which', 'should', 'be', 'known', 'to', 'all', 'man', 'kind', 'or', 'at', 'least', 'to', 'all', 'that', 'exist,', 'ALANiS', 'will', 'always', '\"rock', 'ya\"', 'completely', 'to', 'the', 'end!', 'So', 'does', 'Madonna', 'in', 'this', 'film,', 'and', 'just', 'entirely!', 'Her', 'acting', 'is', 'superb!']\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# load the document\n",
    "filename = 'test/pos/10069_10.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138430\n",
      "[('br', 57141), ('The', 44351), ('movie', 41198), ('film', 37009), ('one', 23188), ('like', 18876), ('This', 14725), ('good', 13927), ('It', 12074), ('would', 12010), ('time', 11504), ('really', 11276), ('story', 11058), ('even', 10882), ('see', 10854), ('much', 9289), ('get', 8996), ('people', 8474), ('bad', 8456), ('great', 8260), ('made', 7887), ('first', 7881), ('well', 7850), ('also', 7722), ('make', 7599), ('films', 7597), ('movies', 7587), ('could', 7567), ('way', 7467), ('dont', 7333), ('But', 7216), ('characters', 7183), ('think', 7078), ('Its', 6738), ('And', 6715), ('seen', 6486), ('character', 6455), ('watch', 6269), ('many', 6233), ('two', 6171), ('plot', 6137), ('acting', 6110), ('never', 6101), ('little', 5981), ('know', 5954), ('In', 5880), ('best', 5717), ('love', 5701), ('show', 5683), ('life', 5651)]\n",
      "35629\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r', encoding=\"utf8\")\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    " \n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    " \n",
    "# leer todos los directorios en el documento\n",
    "def process_docs(directory, vocab):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip files that do not have the right extension\n",
    "\t\tif not filename.endswith(\".txt\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    " \n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w', encoding=\"utf8\")\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('train/neg', vocab)\n",
    "process_docs('train/pos', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "# keep tokens with > 5 occurrence\n",
    "min_occurane = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción del corpus desde el sitio\n",
    "#### gensim doc2vec & IMDB sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "assert os.path.isfile(\"alldata-id-10.txt\"), \"alldata-id.txt unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 docs: 10 train-sentiment, 0 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('alldata-id-10.txt', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurar modelos de evaluación y capacitación de Doc2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las opciones de parámetros siguientes varían:\n",
    "\n",
    "Vectores de 100 dimensiones,  el submuestreo frecuente de palabras parece disminuir la precisión de la predicción de sentimientos, por lo que se omite\n",
    "cbow=0 significa salto-gramo que es equivalente al modo 'PV-DBOW' del papel, emparejado en gensim con dm=0\n",
    "A ese modelo DBOW se le agregan dos modelos DM, uno que promedia los vectores de contexto ( dm_mean) y otro que los concatena (lo que dm_concatda como resultado un modelo mucho más grande, más lento y más hambriento de datos)\n",
    "a min_count=2ahorra bastante memoria del modelo, descartando solo las palabras que aparecen en un solo documento (y, por lo tanto, no son más expresivas que los propios vectores de documentos únicos para cada uno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jess\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "C:\\Users\\Jess\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "C:\\Users\\Jess\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "#assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: testfixtures in c:\\users\\jess\\anaconda3\\lib\\site-packages (6.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jess\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install testfixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de evaluación predictiva \n",
    "Métodos auxiliares para evaluar la tasa de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentimentDocument(words=['it', 'was', 'a', 'bit', 'bizarre', 'and', 'evil', 'and', 'i', 'enjoyed', 'it', 'a', 'lot', ',', 'the', 'characters', 'in', 'the', 'show', 'were', 'great', 'as', 'well', ',', 'and', 'complimented', 'one', 'another', 'well', '.', 'i', 'was', 'sorry', 'to', 'see', 'it', 'cut', 'off', '.', '.', 'i', 'would', 'have', 'loved', 'to', 'see', 'where', 'it', 'could', 'have', 'went', '.', 'you', 'found', 'yourself', 'leaning', 'toward', 'lucas', 'buck', 'the', 'sheriff', 'who', 'had', 'more', 'secrets', 'than', 'anyone', '.', 'lucas', 'was', 'frightening', 'and', 'alluring', '.', 'and', 'i', 'would', 'have', 'liked', 'to', 'have', 'seen', 'more', 'of', 'him', 'and', 'how', 'his', 'character', 'became', '.', 'i', 'will', 'however', 'buy', 'the', 'show', 'just', 'to', 'enjoy', ',', 'it', 'was', 'great', 'to', 'something', 'different', 'on', 'tv', '.', 'and', 'paige', 'turrco', 'who', 'was', \"caleb's\", 'cousin', ',', 'she', 'was', 'a', 'big', 'mystery', 'as', 'to', 'where', 'and', 'what', 'she', 'meant', 'to', 'lucas', '.', 'its', 'a', 'shame', 'it', \"isn't\", 'around', 'still', '.', '.', 'or', 'was', 'never', 'finished', ',', 'i', 'would', 'have', 'loved', 'to', 'see', 'what', 'would', 'have', 'happened', '.'], tags=[0], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['without', 'peter', 'ustinov', 'and', 'maggie', 'smith', ',', 'this', 'could', 'easily', 'have', 'been', 'a', 'turkey', '.', 'but', 'they', 'are', 'brilliant', '.', 'ustinov', 'is', 'at', 'his', 'best', ',', 'and', 'for', 'fans', 'of', 'maggie', ',', 'it', 'is', 'great', 'to', 'see', 'her', 'in', 'her', 'early', 'days', ',', 'matching', 'ustinov', 'every', 'step', 'of', 'the', 'way', 'for', 'with', 'and', 'timing', '.', 'for', 'englishmen', 'in', 'their', 'fifties', '(', 'and', 'i', 'am', 'in', 'that', 'bracket', ')', ',', 'it', 'is', 'always', 'entertaining', 'to', 'see', 'glimpses', 'of', 'and', 'hear', 'sounds', 'of', 'the', 'swinging', 'sixties', ',', 'and', 'although', 'this', 'film', 'spends', 'a', 'lot', 'of', 'time', 'in', 'offices', ',', 'it', 'has', 'plenty', 'of', 'sixties', 'nostalgia', ',', 'including', 'red', 'buses', ',', 'carnaby', 'street', ',', 'a', 'song', 'by', 'lulu', 'and', 'a', 'delicious', 'shot', 'up', 'the', 'micro-skirt', 'of', 'a', 'waitress', ',', 'the', 'like', 'of', 'which', 'england', 'has', 'never', 'seen', 'since', 'in', 'public', 'places', '.', 'as', 'an', 'i', '.', 't', '.', 'engineer', ',', 'i', 'know', 'that', 'the', 'computer', 'hacking', 'tricks', 'are', 'laughable', ',', 'but', 'they', 'are', 'not', 'meant', 'to', 'be', 'taken', 'seriously', '.', 'nor', 'are', 'the', 'wonderful', 'stereotypes', 'of', 'italians', ',', 'french', 'and', 'germans', '.'], tags=[1], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['butch', 'the', 'peacemaker', '?', 'evidently', '.', 'after', 'the', 'violent', 'beginning', 'with', 'spike', ',', 'tom', 'and', 'jerry', 'all', 'swinging', 'away', 'at', 'each', 'other', ',', 'butch', 'calls', 'a', 'halt', 'and', 'wants', 'to', 'know', 'why', '.', \"it's\", 'a', 'good', 'question', '.', '\"', 'cats', 'can', 'get', 'along', 'with', 'dogs', ',', \"can't\", 'they', '?', '\"', 'he', 'asks', 'tom', ',', 'who', 'nods', 'his', 'head', 'in', 'agreement', '.', '\"', 'mice', 'can', 'get', 'along', 'with', 'cats', ',', 'right', '?', '\"', 'jerry', 'nods', '\"', 'no', ',', '\"', 'and', 'then', 'sees', 'that', \"isn't\", 'the', 'right', 'answer', '.', 'they', 'go', 'inside', 'and', 'butch', 'draws', 'up', 'a', '\"', 'peace', 'treaty', '\"', '(', 'complete', 'with', 'professional', 'artwork', '!', ')', '.', 'most', 'of', 'the', 'rest', ',', 'and', 'the', 'bulk', 'of', 'the', 'cartoon', ',', 'is', 'the', 'three', 'of', 'them', 'being', 'extremely', 'nice', 'to', 'one', 'another', 'what', 'a', 'refreshing', 'change-of-pace', '.', 'i', 'found', 'it', 'fun', 'to', 'watch', '.', 'i', 'can', 'a', 'million', 'of', 'these', 'cartoons', 'in', 'which', 'every', 'beats', 'each', 'other', 'over', 'the', 'head', '.', 'anyway', ',', 'you', 'knew', 'the', 'peace', \"wasn't\", 'going', 'to', 'last', '.', 'a', 'big', 'piece', 'of', 'steak', 'spells', 'the', 'death', 'of', 'the', '\"', 'peace', 'treaty', '\"', 'but', 'en', 'route', 'it', 'was', 'nice', 'change', 'and', 'still', 'had', 'some', 'of', 'usual', 'tom', '&', 'jerry', 'clever', 'humor', '.'], tags=[2], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['somehow', ',', 'this', 'movie', 'manages', 'to', 'be', 'invigorating', ',', 'bittersweet', ',', 'and', 'heartwarming', 'at', 'the', 'same', 'time', '.', 'stars', 'like', 'tony', 'shalhoub', '(', 'from', 'providence', ')', 'bring', 'the', 'tale', 'to', 'life', '.', 'the', 'story', 'itself', 'is', 'inspiring', '.', 'we', 'see', 'a', 'desperate', ',', 'up-and-down', 'life', 'through', 'the', 'most', 'innocent', 'eyes', 'imaginable', ':', 'a', \"bird's\", '.', 'paulie', 'begins', 'his', 'life', 'as', 'a', 'baby', 'parrot', 'given', 'to', 'a', 'little', 'girl', '(', 'played', 'by', 'hallie', 'eisenberg', ',', 'also', 'known', 'as', 'the', 'pepsi', 'girl', ')', 'with', 'a', 'speech', 'impediment', '.', 'while', 'she', 'learns', 'to', 'speak', 'correctly', ',', 'so', 'does', 'paulie', '.', 'however', ',', 'unlike', 'most', 'birds', ',', 'he', 'can', 'speak', 'and', 'understand', 'everything', 'being', 'said', '.', 'the', 'military', 'father', \"doesn't\", 'like', 'the', 'bird', ',', 'so', 'he', 'is', 'sent', 'to', 'a', 'pawn', 'shop', 'and', 'bought', 'by', 'an', 'aging', 'artist', ',', 'ivy', '.', 'she', 'teaches', 'him', 'manners', ',', 'etc', '.', ',', 'while', 'traveling', 'across', 'the', 'country', 'to', 'find', \"paulie's\", 'owner', '.', 'the', 'movie', 'continues', 'with', 'several', 'twists', 'of', 'fate', ',', 'until', 'paulie', 'ends', 'up', 'at', 'a', 'laboratory', 'where', 'he', 'is', 'eventually', 'hidden', 'away', 'in', 'a', 'basement', ',', 'and', 'found', 'by', 'a', 'russian', 'custodian', ',', 'who', 'is', 'touched', 'by', 'the', \"bird's\", 'story', '.', 'the', 'plot', 'is', 'in', 'keeping', 'with', 'the', 'simple', ',', 'metaphorical', 'theme', 'that', 'language', 'is', 'a', 'gift', ',', 'and', 'a', 'curse', '.', 'i', 'would', 'like', 'to', 'say', 'that', 'the', 'soundtrack', 'is', 'astounding', '.', 'a', 'beautiful', 'mixture', 'of', 'flute', ',', 'digital', 'base', ',', 'and', 'horns', 'enhance', 'the', 'movie', 'to', 'the', 'point', 'of', 'pure', 'ecstasy', '.', 'the', 'sweeping', 'camera', 'angles', 'and', 'breathtaking', 'scenery', 'beautify', 'the', 'story', 'even', 'more', '.', 'and', ',', 'as', 'a', 'final', 'remark', ',', 'the', 'puppetry', 'is', 'entirely', 'believable', '.', '(', 'unlike', 'in', 'star', 'wars', ',', 'where', 'yoda', 'resembles', 'a', 'muppet', ')', 'this', 'film', 'is', 'one', 'of', 'my', 'favorite', 'movies', ',', 'with', 'the', 'added', 'remark', 'that', 'my', 'wonderful', 'parakeet', 'of', 'four', 'years', 'died', 'recently', '.', 'overall', ',', 'i', 'give', 'this', 'movie', '****', 'out', 'of', 'four', 'stars', ',', 'two', 'thumbs', 'up', ',', 'and', 'a', 'big', 'hug', '.'], tags=[3], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['rachel', 'mcadams', '.', 'cillian', 'murphy', '.', 'wes', 'craven', '.', 'the', 'dream', 'team', '.', 'this', 'is', 'one', 'of', 'the', 'best', 'thrillers', 'of', '2005', '.', 'a', 'great', 'plot', '.', 'a', 'great', 'twist', '.', 'a', 'great', 'eye', 'candy', '.', 'this', 'is', 'one', 'of', 'wes', \"craven's\", 'greatest', 'movies', 'apart', 'from', '\"', 'scream', '\"', 'and', '\"', 'a', 'nightmare', 'on', 'elm', 'street', '\"', '.', 'this', 'could', 'be', 'the', 'best', '.', 'the', 'plot', 'is', 'one', 'of', 'the', 'best', 'things', 'about', 'the', 'movie', 'and', 'it', 'is', 'very', 'simple', '.', 'lisa', 'reisert', '(', 'mcadams', ')', 'a', 'struggling', 'hotel', 'manager', ',', 'boards', 'a', 'late', 'red', 'eye', 'back', 'to', 'la', '.', 'little', 'does', 'she', 'know', 'that', 'she', 'has', 'been', 'followed', 'by', 'jackson', 'ripner', 'aka', 'jack', 'the', 'ripper', '(', 'murphy', ')', '.', 'they', 'have', 'a', 'couple', 'of', 'drinks', 'and', 'end', 'up', 'sitting', 'together', 'on', 'the', 'plane', '.', 'later', 'he', 'reveals', 'to', 'her', 'that', 'he', 'is', 'an', 'assassin', 'who', 'was', 'sent', 'to', 'kill', 'the', 'secreteary', 'of', 'homeland', 'security', 'who', 'is', 'staying', 'in', \"lisa's\", 'hotel', '.', 'and', 'what', 'does', 'this', 'have', 'to', 'with', 'lisa', '.', 'well', ',', 'she', 'would', 'have', 'to', 'move', 'his', 'room', 'to', 'the', 'top', 'suite', 'so', 'they', 'can', 'bomb', 'him', ',', 'if', 'she', \"doesn't\", 'ripner', 'will', 'murder', 'her', 'father', '(', 'brian', 'cox', ')', '.', 'the', 'twist', 'and', 'bringing', 'of', 'the', 'story', 'really', 'gives', 'it', 'the', 'extra', 'zing', 'and', 'the', 'side', 'characters', 'and', 'the', 'side', 'jokes', 'really', 'add', 'to', 'it', '.', 'overall', ',', 'definitely', 'one', 'of', 'the', 'best', 'thrillers', 'of', '2005', '.', 'definitely', 'worth', 'the', 'see', '.', '3', '1/2', 'out', 'of', '4', 'stars'], tags=[4], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['halloween', 'is', 'not', 'only', 'the', 'godfather', 'of', 'all', 'slasher', 'movies', 'but', 'the', 'greatest', 'horror', 'movie', 'ever', '!', 'john', 'carpenter', 'and', 'debra', 'hill', 'created', 'the', 'most', 'suspenseful', ',', 'creepy', ',', 'and', 'terrifying', 'movie', 'of', 'all', 'time', 'with', 'this', 'classic', 'chiller', '.', 'michael', 'myers', 'is', 'such', 'a', 'phenomenal', 'monster', 'in', 'this', 'movie', 'that', 'he', 'inspired', 'scores', 'of', 'imitators', ',', 'such', 'as', 'jason', 'vorhees', '(', 'friday', 'the', '13th', ')', ',', 'the', 'miner', '(', 'my', 'bloody', 'valentine', ')', ',', 'and', 'charlie', 'puckett', '(', 'the', 'night', 'brings', 'charlie', ')', '.', 'okay', ',', 'so', 'i', 'got', 'a', 'little', 'obscure', 'there', ',', 'but', 'it', 'just', 'goes', 'to', 'show', 'you', 'the', 'impact', 'that', 'this', 'movie', 'had', 'on', 'the', 'entire', 'horror', 'genre', '.', 'no', 'longer', 'did', 'a', 'monster', 'have', 'to', 'come', 'from', 'king', \"tut's\", 'tomb', 'or', 'from', 'dr', '.', \"frankenstein's\", 'lab', '.', 'he', 'could', 'be', 'created', 'in', 'the', 'cozy', 'little', 'neighborhoods', 'of', 'suburbia', '.', 'and', 'on', 'the', 'night', 'he', 'came', 'home', '.', '.', '.', 'haddonfield', ',', 'illinois', 'and', 'the', 'viewers', 'would', 'never', 'be', 'the', 'same', '.', 'there', 'are', 'many', 'aspects', 'of', 'this', 'movie', 'that', 'make', 'it', 'the', 'crowning', 'jewel', 'of', 'horror', 'movies', '.', 'first', 'is', 'the', 'setting', '.', '.', '.', 'it', 'takes', 'place', 'in', 'what', 'appears', 'to', 'be', 'a', 'normal', 'suburban', 'neighborhood', '.', 'many', 'of', 'us', 'who', 'grew', 'up', 'in', 'an', 'area', 'such', 'as', 'this', 'can', 'easily', 'identify', 'with', 'the', 'characters', '.', 'this', 'is', 'the', 'type', 'of', 'neighborhood', 'where', 'you', 'feel', 'safe', ',', 'but', 'if', 'trouble', 'starts', 'to', 'brew', ',', 'nobody', 'wants', 'to', 'lift', 'a', 'finger', 'to', 'get', 'involved', '(', 'especially', 'when', 'a', 'heavy-breathing', 'madman', 'is', 'trying', 'to', 'skewer', 'our', 'young', 'heroine', '.', ')', 'along', 'with', 'the', 'setting', ',', 'the', 'movie', 'takes', 'place', 'on', 'halloween', '!', '!', 'the', 'scariest', 'night', 'of', 'the', 'year', '!', 'while', 'most', 'people', 'are', 'carving', 'jack-o-lanterns', ',', 'michael', 'myers', 'is', 'looking', 'to', 'carve', 'up', 'some', 'teenie-boppers', '.', 'besides', 'the', 'setting', ',', 'there', 'is', 'some', 'great', 'acting', '.', 'jamie', 'lee', 'curtis', 'does', 'a', 'serviceable', 'job', 'as', 'our', 'heroine', ',', 'laurie', 'strode', ',', 'a', 'goody-two-shoes', 'high-schooler', 'who', 'can', 'never', 'seem', 'to', 'find', 'a', 'date', '.', 'however', ',', 'it', 'is', 'donald', 'pleasance', ',', 'as', 'dr', '.', 'sam', 'loomis', ',', 'who', 'really', 'steals', 'the', 'show', '.', 'his', 'portrayal', 'of', 'the', 'good', 'doctor', ',', 'who', 'knows', 'just', 'what', 'type', 'of', 'evil', 'hides', 'behind', 'the', 'black', 'eyes', 'of', 'michael', 'myers', 'and', 'feels', 'compelled', 'to', 'send', 'him', 'to', 'hell', 'once', 'and', 'for', 'all', ',', 'is', 'the', 'stuff', 'of', 'horror', 'legend', '.', 'however', ',', 'it', 'is', 'the', 'synthesizer', 'score', 'that', 'really', 'drives', 'this', 'picture', 'as', 'it', 'seems', 'to', 'almost', 'put', 'the', 'viewer', 'into', 'the', 'film', '.', 'once', 'you', 'hear', 'it', ',', 'you', 'will', 'never', 'forget', 'it', '.', 'i', 'also', 'enjoy', 'the', 'grainy', 'feel', 'to', 'this', 'picture', '.', 'nowadays', ',', 'they', 'seem', 'to', 'sharpen', 'up', 'the', 'image', 'of', 'every', 'movie', ',', 'giving', 'us', 'every', 'possible', 'detail', 'of', 'the', 'monster', 'we', 'are', 'supposed', 'to', 'be', 'afraid', 'of', '.', 'in', 'halloween', ',', 'john', 'carpenter', 'never', 'really', 'lets', 'us', 'get', 'a', 'complete', 'look', 'at', 'michael', 'myers', '.', 'he', 'always', 'seems', 'like', 'he', 'is', 'a', 'part', 'of', 'the', 'shadows', ',', 'and', ',', 'i', 'think', 'that', 'is', 'what', 'makes', 'him', 'so', 'terrifying', '.', 'there', 'are', 'many', 'scenes', 'where', 'michael', 'is', 'partly', 'visible', 'as', 'he', 'spies', 'on', 'the', 'young', 'teens', '(', 'unbeknownst', 'to', 'them', ')', ',', 'which', 'adds', 'to', 'his', 'creepiness', '.', 'if', 'you', 'think', 'about', ',', 'some', 'wacko', 'could', 'be', 'watching', 'you', 'right', 'now', 'and', 'you', \"wouldn't\", 'even', 'know', 'it', '.', 'unfortunately', 'for', 'our', 'teenagers', '(', 'and', 'fortunately', 'for', 'us', 'horror', 'fans', ')', ',', 'when', 'they', 'find', 'michael', ',', \"he's\", 'not', 'looking', 'for', 'candy', 'on', 'this', 'halloween', 'night', '.', '.', \"he's\", 'looking', 'for', 'blood', '.', 'finally', ',', 'michael', 'myers', ',', 'himself', ',', 'is', 'a', 'key', 'element', 'to', 'this', \"movie's\", 'effectiveness', '.', 'his', 'relentless', 'pursuit', 'of', 'laurie', 'strode', 'makes', 'him', 'seem', 'like', 'the', 'killer', 'who', 'will', 'never', 'stop', '.', 'he', 'is', 'the', 'bogeyman', 'that', 'will', 'haunt', 'you', 'for', 'the', 'rest', 'of', 'your', 'life', '.', 'so', ',', 'if', 'you', 'have', 'not', 'seen', 'this', 'movie', '(', 'if', 'there', 'are', 'still', 'some', 'of', 'you', 'out', 'there', 'who', \"haven't\", ',', 'or', 'even', 'if', 'you', 'have', ')', ',', 'grab', 'some', 'popcorn', ',', 'turn', 'off', 'every', 'light', ',', 'pop', 'this', 'into', 'the', 'old', 'dvd', 'and', 'watch', 'in', 'fright', '.', 'trick', 'or', 'treat', '!'], tags=[5], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['with', 'lots', 'of', 'sunshine', ',', 'gauzy', 'light', 'and', 'shadow', 'filtering', 'through', 'windows', 'and', 'into', 'rooms', ',', 'tracking', 'shots', 'moving', 'through', 'crowds', 'with', 'hand-held', 'camera', ',', 'quick-paced', 'editing', 'and', 'extreme', 'close-ups', 'here', 'and', 'there', ',', 'the', 'photography', 'is', 'the', 'thing', 'in', 'this', 'interesting', ',', 'artistically', 'done', 'film', '.', 'the', 'plot', 'of', 'this', 'film', 'starts', 'out', 'as', 'a', 'bit', 'of', 'fluff', 'about', 'a', 'beauty', 'contest', '.', 'the', 'film', 'begins', 'on', 'a', 'warm', 'sunday', 'at', 'the', 'local', 'swimming', 'pool', ',', 'where', 'we', 'meet', 'the', 'lovely', 'lucienne', 'aka', 'lulu', '(', 'played', 'by', 'louise', 'brooks', ')', '-', 'a', 'bit', 'of', 'a', 'show-off', 'in', 'front', 'of', 'the', 'gawking', 'men', 'by', 'poolside', ',', 'she', 'soon', 'decides', 'to', 'enter', 'herself', 'to', 'represent', 'france', 'in', 'the', 'miss', 'europe', 'beauty', 'contest', ',', 'much', 'to', 'the', 'chagrin', 'of', 'her', 'very', 'jealous', ',', 'stick-in-the-mud', 'fiancé', '(', 'a', 'pretty', 'annoying', 'fellow', ',', 'really', ')', '.', 'strutting', 'down', 'the', 'runway', 'the', 'ten', 'contestants', 'display', 'themselves', 'in', 'swimsuits', ',', 'while', 'the', 'winner', 'is', 'chosen', 'as', 'the', 'contestant', 'who', 'receives', 'the', 'longest', 'applause', '(', 'i', 'was', 'wondering', ',', \"couldn't\", 'the', 'girls', 'just', 'walk', 'slower', 'to', 'prolong', 'their', 'length', 'of', 'time', '-', 'and', 'thus', 'applause', '-', 'on', 'the', 'catwalk', '?', '!', ')', '.', 'lulu', 'is', 'soon', 'being', 'chased', 'by', 'a', 'prince', 'and', 'a', 'maharaja', ',', 'but', 'her', 'hot-headed', 'beau', \"doesn't\", 'like', 'the', 'attentions', 'paid', 'to', 'her', 'by', 'other', 'men', 'or', 'her', 'adoring', 'public', ',', 'for', 'that', 'matter', '(', 'i', 'guess', 'he', 'just', 'wants', 'her', 'in', 'his', 'house', ',', 'cooking', 'his', 'meals', ',', 'and', 'staying', 'out', 'of', 'sight', ',', 'eh', '?', '!', ')', '.', 'louise', 'brooks', 'is', 'beautiful', 'and', 'charming', ',', 'her', 'presence', 'helps', 'enhance', 'this', 'film', ',', 'but', \"it's\", 'really', 'the', 'way', 'it', 'is', 'photographed', 'that', 'held', 'my', 'interest', 'the', 'most', '.', 'a', 'bit', 'distracting', 'is', 'the', 'odd', 'dubbed', 'sound', ',', 'which', 'is', 'a', 'bit', 'off', '.', 'the', 'print', 'on', 'this', 'version', 'looked', 'very', 'clear', 'and', 'full', 'of', 'nice', 'contrast', 'though', '.', 'watching', 'this', 'i', 'just', 'tried', 'to', 'overlook', 'the', 'sound', 'problems', 'and', 'watch', 'the', 'film', 'visually', ',', 'and', 'i', 'found', 'the', 'movie', 'to', 'be', 'excellent', ',', 'well', 'worth', 'seeing', '.'], tags=[6], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['this', 'is', 'one', 'of', 'my', 'favorite', 'mr', '.', 'motos', ',', 'and', 'i', 'have', 'seen', 'them', 'all', '.', 'as', 'usual', 'lorre', 'is', 'his', 'charming', 'self', 'as', 'the', 'debonair', 'mr', '.', 'moto', '.', 'lionel', 'atwill', 'plays', 'a', 'delightfully', 'zany', 'museum', 'curator', ',', 'the', 'usual', 'comic', 'relief', 'is', 'quite', 'funny', 'here', ',', 'and', 'there', 'are', 'lots', 'of', 'suspects', 'on', 'whom', 'to', 'cast', 'an', 'eye', '.', \"it's\", 'fast', 'paced', 'and', 'fun', '.', 'the', 'archaeologist', \"doesn't\", 'have', 'quite', 'the', 'same', 'flair', 'as', 'thomas', 'beck', ',', 'the', 'usual', 'second', 'lead', 'in', 'these', 'programmers', ',', 'but', \"he's\", 'adequate', '.', 'stepin', 'fetchit', 'is', 'on', 'board', ',', 'and', 'while', 'he', 'speaks', 'in', 'a', 'stereotypical', 'manner', 'his', 'lines', 'are', 'funny', ',', 'not', 'demeaning', 'to', 'his', 'intelligence', ',', 'and', 'he', 'actually', 'saves', 'the', 'day', 'in', 'his', 'brief', 'time', 'on', 'screen', '.'], tags=[7], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['there', 'are', 'some', 'things', 'i', 'will', 'never', 'understand', ';', 'why', 'underwear', 'comes', 'in', 'packs', 'of', 'threes', 'when', 'clearly', 'thats', 'not', 'enough', 'is', 'an', 'example', '.', 'similarly', ',', 'i', 'will', 'never', 'understand', 'this', 'film', ',', 'and', 'that', 'is', 'brilliant', '.', 'if', 'you', 'approach', 'this', 'film', 'expecting', 'an', 'actual', 'movie', ',', 'you', 'might', 'as', 'well', 'be', 'approaching', 'satan', 'expecting', 'a', 'hug', ';', 'although', 'that', 'may', 'well', 'be', 'possible', 'if', 'you', 'greet', 'this', \"film's\", 'satanic', 'figures', '.', 'take', 'pitch', 'for', 'instance', ';', 'the', 'most', 'ineffectual', ',', 'camp', ',', 'unhellish', 'portrayal', 'of', 'a', 'devil', 'since', 'freddy', 'mercury', 'and', 'wayne', 'sleep', 'joined', 'forces', 'to', 'create', 'a', 'ten', 'foot', 'satan', 'costume', 'from', 'red', 'body', 'paint', 'and', 'horns', 'covered', 'with', 'condoms', '.', 'however', ',', 'it', 'does', 'create', 'some', 'of', 'the', 'most', 'hilarious', 'moments', 'of', 'any', 'film', 'ever', '.', 'seriously', ',', 'this', 'is', 'no', 'understatement', '.', 'the', 'same', 'can', 'be', 'applied', 'to', 'every', 'other', 'character', ',', 'bar', 'the', 'little', 'girl', 'who', 'acts', 'so', 'sickly', 'innocent', \"she's\", 'probably', 'overcompensating', 'for', 'some', 'serious', 'crime', \"she's\", 'part', 'of', '.', 'then', 'again', ',', 'if', \"santa's\", 'inter-space', 'recon', 'station', 'is', 'real', ',', 'there', 'is', 'no', 'chance', 'she', 'could', 'have', 'avoided', 'him', 'this', 'long', '.', 'put', 'simply', ',', 'if', 'you', \"haven't\", 'seen', 'this', 'movie', ',', 'you', 'cannot', 'consider', 'yourself', 'a', 'serious', 'buff', '.', 'the', 'achingly', 'funny', 'characterisation', ',', 'acting', ',', 'concept', ',', 'and', 'almost-under-the-radar', 'racism', 'makes', 'this', 'a', 'must', 'see', 'above', 'any', 'film', 'to', 'date', '(', 'if', \"you're\", 'after', 'pure', 'laughter', 'that', 'is', ')', '.'], tags=[8], split='train', sentiment=1.0),\n",
       " SentimentDocument(words=['this', 'was', 'my', 'favourite', 'film', 'as', 'a', 'child', ',', 'and', 'i', 'have', 'been', 'in', 'the', 'stage', 'production', 'a', 'few', 'times', 'so', 'it', 'will', 'always', 'remain', 'my', 'favourite', 'muscical', 'and', 'i', 'doubt', 'anybody', 'could', 'ever', 're-make', 'the', 'story', 'of', 'oliver', 'twist', 'on', 'screen', ',', 'any', 'better', 'than', 'this', 'one', 'did', '.', 'my', 'all-time', 'favourite', \"''bad\", \"guy''\", 'has', 'to', 'be', 'oliver', 'reed', 'as', 'bill', 'sikes', '.', 'not', 'only', 'did', 'he', 'scare', 'the', 'life', 'out', 'of', 'my', 'when', 'i', 'watched', 'it', 'as', 'a', '6', 'year', 'old', ',', 'but', 'now', 'as', 'a', 'woman', 'i', 'can', 'empathize', 'more', 'with', 'nancys', 'character', ',', 'the', 'bar', 'maid/prostitute', 'who', 'helps', 'oliver', 'get', 'the', 'life', 'he', 'deserves', '.', 'jack', 'wilde', 'as', 'the', 'artful', 'dodger', ',', 'was', 'fantastic', ',', 'and', 'i', \"don't\", 'think', 'anybody', 'could', 'ever', 'out-do', 'him', ',', 'as', 'the', 'street-pocket', 'picker', ',', 'and', 'best', 'friend', 'of', 'fagin', '.', 'the', 'music', 'is', 'fantastic', ',', 'especially', \"fagin's\", 'numbers', ',', \"i'm\", 'also', 'quite', 'thankful', 'they', \"didn't\", 'give', 'bill', 'sikes', 'a', 'musical', 'number', ',', 'it', \"wouldn't\", 'of', 'worked', 'with', 'him', 'being', 'such', 'a', 'sinister', 'character', '.', 'i', 'think', 'carol', 'reed', 'did', 'an', 'excellent', 'job', 'of', \"nancy's\", 'sticky', 'ending', ',', 'keeping', 'it', 'a', 'g', 'rated', 'movie', 'by', 'disguising', 'her', 'beating', ',', 'but', 'giving', 'enough', 'away', 'to', 'show', 'the', 'violence', 'of', 'bill', 'towards', 'her', '.', 'this', 'movie', 'is', 'both', 'charming', ',', 'and', 'charismatic', 'as', 'a', 'musical', 'sing-along', ',', 'as', 'well', 'as', 'being', 'a', 'moving', 'drama', 'that', 'follows', 'a', 'young', 'boy', 'as', 'he', 'tries', 'to', 'find', 'where', 'he', 'belongs', 'in', 'life', '.'], tags=[9], split='train', sentiment=1.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-08-25 14:15:05.650435\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You must specify either total_examples or total_words, for proper job parameters updationand progress calculations. The usual value is total_examples=model.corpus_count.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-5214fa7c82bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0melapsed_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0melapsed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[1;31m#duration = '%.1f' % elapsed()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtotal_words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1186\u001b[1;33m                 \u001b[1;34m\"You must specify either total_examples or total_words, for proper job parameters updation\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m                 \u001b[1;34m\"and progress calculations. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m                 \u001b[1;34m\"The usual value is total_examples=model.corpus_count.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You must specify either total_examples or total_words, for proper job parameters updationand progress calculations. The usual value is total_examples=model.corpus_count."
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        \n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list)\n",
    "            #duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se ejecuta por el problema del coteo de palabras para que el job cuente con una ejecución inicial y final.\n",
    "Ya no lo logré arreglar.\n",
    "\n",
    "## Intente con otro ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['i', 'love', 'hot', 'chocolate', '.'], tags=['0']), TaggedDocument(words=['i', 'hate', 'hot', 'chocolate'], tags=['1']), TaggedDocument(words=['i', 'love', 'hot', 'tea', '.'], tags=['2']), TaggedDocument(words=['i', 'love', 'hot', 'cake', '.'], tags=['3'])]\n",
      "iteration 0\n",
      "Model Ready\n",
      "\n",
      "Test: I adore hot chocolate\n",
      "\n",
      "\t(score: 0.6380) I hate hot chocolate\n",
      "\t(score: 0.5394) I love hot chocolate.\n",
      "\t(score: 0.2405) I love hot tea.\n",
      "\t(score: 0.1522) I love hot cake.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "data = [\"I love hot chocolate.\",\n",
    "        \"I hate hot chocolate\",\n",
    "       \"I love hot tea.\",\n",
    "       \"I love hot cake.\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "print(tagged_data)\n",
    "#Train and save\n",
    "max_epochs = 10\n",
    "vec_size = 5\n",
    "alpha = 0.025\n",
    "\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size, #it was size earlier\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if epoch % 10 == 0:\n",
    "        print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs) #It was model.iter earlier\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "print(\"Model Ready\")\n",
    "\n",
    "test_sentence=\"I adore hot chocolate\"\n",
    "test_data = word_tokenize(test_sentence.lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "#print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "sims = model.docvecs.most_similar([v1])\n",
    "print(\"\\nTest: %s\\n\" %(test_sentence))\n",
    "for indx, score in sims:\n",
    "    print(\"\\t(score: %.4f) %s\" %(score, data[int(indx)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
