{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINERÍA DE TEXTOS\n",
    "\n",
    "## Alumna: Jessica Sarahi Méndez Rincón\n",
    "\n",
    "\n",
    "###    Tarea 3\n",
    "### Clase 3 Tarea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando El ramo azul, de Octavio Paz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\phyton 372\\lib\\site-packages (0.48.0)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in c:\\phyton 372\\lib\\site-packages (from numba) (0.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\phyton 372\\lib\\site-packages (from numba) (40.6.2)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\phyton 372\\lib\\site-packages (from numba) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numba\n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "seaborn.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in c:\\phyton 372\\lib\\site-packages (0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import docx2txt\n",
    "my_text = docx2txt.process(\"ElRamoAzul.docx\")\n",
    "#print(my_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "azul_tokens = nltk.word_tokenize(my_text)\n",
    "text = nltk.Text(azul_tokens)\n",
    "azul_series = pandas.Series(azul_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuantas palabras hay en el texto? (descartando símbolos de puntuación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Ramo Azul tiene 996 tokens.\n"
     ]
    }
   ],
   "source": [
    "def ejemplo_uno():\n",
    "     \"\"\"Cuenta los tokens y retorna un entero\n",
    "     \"\"\"\n",
    "     # or alternatively len(text1)\n",
    "     return len(azul_tokens)\n",
    "\n",
    "AZUL_TOKEN_COUNT = ejemplo_uno()\n",
    "print(\"El Ramo Azul tiene {:,} tokens.\".format(\n",
    "     AZUL_TOKEN_COUNT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cuántas palabras diferentes hay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Ramo Azul tiene 477 únicos tokens.\n"
     ]
    }
   ],
   "source": [
    "def ejemplo_dos():\n",
    "    \"\"\"Cuenta los tokens únicos y retorna un entero\n",
    "    \"\"\"\n",
    "    # or alternatively len(set(text1))\n",
    "    return len(set(nltk.word_tokenize(my_text)))\n",
    "\n",
    "AZUL_UNIQUE_COUNT = ejemplo_dos()\n",
    "print(\"El Ramo Azul tiene {:,} únicos tokens.\".format(\n",
    "    AZUL_UNIQUE_COUNT))\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Después de lematizar las palabras, cuantas palabras diferentes hay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Ramo Azul tiene 477 lemma (encontrados en  WordNet).\n"
     ]
    }
   ],
   "source": [
    "def ejemplo_tres():\n",
    "    \"\"\"Cuenta las palabras en base a su lematización y retorna un número entero\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return len(set([lemmatizer.lemmatize(w,'v') for w in text]))\n",
    "\n",
    "AZUL_LEMMA_COUNT = ejemplo_tres()\n",
    "print(\"El Ramo Azul tiene {:,} lemma (encontrados en  WordNet).\".format(\n",
    "    AZUL_LEMMA_COUNT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cuál es la diversidad léxica del texto dado? (relación de palabras únicas con respecto al número total de palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "\n",
    "def diversidad_lexica(tokens):\n",
    "    \"\"\"Calcula la diversidad lexica de una lista de tokens y Retorna un número float (fraccionario)\n",
    "    \"\"\"\n",
    "    return len(set(tokens))/float(len(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La diversidad Léxica de el Ramo Azul es : 0.48\n"
     ]
    }
   ],
   "source": [
    "def respuesta1():\n",
    "    \"\"\"Cálcula la diversidad léxica de un texto en esté caso de el Ramo azul y Retorna un número float (fraccionario) \n",
    "    \"\"\"\n",
    "    return diversidad_lexica(azul_tokens)\n",
    "\n",
    "output = respuesta1()\n",
    "\n",
    "print(\"La diversidad Léxica de el Ramo Azul es : {:.2f}\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cuáles son las 20 palabras (únicas) más frecuentes en el texto? ¿Cuál es su frecuencia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "azul_frequencies = FreqDist(azul_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 96), (',', 59), ('de', 36), ('la', 32), ('y', 18), ('el', 18), ('un', 16), ('me', 15), ('los', 15), ('ojos', 13), ('a', 11), ('que', 10), ('una', 10), ('Me', 9), ('con', 9), ('no', 8), ('?', 8), ('No', 8), ('las', 7), ('se', 7)]\n"
     ]
    }
   ],
   "source": [
    "def respuesta2():\n",
    "    \"\"\"finds 20 most requently occuring tokens\n",
    "\n",
    "    Returns:\n",
    "     list: (token, frequency) for top 20 tokens\n",
    "    \"\"\"\n",
    "    return azul_frequencies.most_common(20)\n",
    "\n",
    "print(respuesta2())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caminé', 'Encendí', 'acerqué', 'azules', 'cabeza', 'cigarrillo', 'detuve', 'machete', 'palabra', 'pueblo', 'puerta', 'párpados', 'quemaba', 'quieres', 'rostro', 'sentía', 'vuelta']\n"
     ]
    }
   ],
   "source": [
    "azul_frequency_frame = pandas.DataFrame(azul_frequencies.most_common(),\n",
    "                                        columns=[\"token\", \"frequency\"])\n",
    "def respuesta3():\n",
    "    \"\"\"gets tokens with length > 5, frequency > 150\"\"\"\n",
    "    frame =  azul_frequency_frame[(azul_frequency_frame.frequency > 1)\n",
    "                                  & (azul_frequency_frame.token.str.len() > 5)]\n",
    "    return sorted(frame.token)\n",
    "\n",
    "output = respuesta3()\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cuál es el número promedio de palabras por oración?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Npumero promedio de tokens por oración: 9.49\n"
     ]
    }
   ],
   "source": [
    "def respuesta4():\n",
    "    \"\"\"average number of tokens per sentence\"\"\"\n",
    "    sentences = sent_tokenize(my_text)\n",
    "    counts = (len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    return sum(counts)/float(len(sentences))\n",
    "\n",
    "output = respuesta4()\n",
    "print(\"Npumero promedio de tokens por oración: {:.2f}\".format(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cuál es la frecuencia de de sustantivos, adjetivos y verbos en el texto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jess\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    " import nltk\n",
    " nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " azul_words = azul_frequency_frame[azul_frequency_frame.token.str.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 parts of speech: [('NN', 283), ('NNP', 54), ('JJ', 38), ('VBP', 19), ('NNS', 18)]\n"
     ]
    }
   ],
   "source": [
    " def respuesta5():\n",
    "    \"\"\"gets the 5 most frequent parts of speech\n",
    "\n",
    "    Returns:\n",
    "     list (Tuple): (part of speech, frequency) for top 5\n",
    "    \"\"\"\n",
    "    tags = nltk.pos_tag(azul_words.token)\n",
    "    frequencies = FreqDist([tag for (word, tag) in tags])\n",
    "    return frequencies.most_common(5)\n",
    "\n",
    "output = respuesta5()\n",
    "print(\"Top 5 parts of speech: {}\".format(output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacer una gramática que analice las tres primeras oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "texto = nltk.word_tokenize(\"Alice loves Bob\")\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    " S -> NP VP\n",
    " VP -> V NP\n",
    " NP -> 'Alice' | 'Bob'\n",
    " V -> 'loves'\n",
    " \"\"\")\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(texto)\n",
    "for tree in trees:\n",
    " print (tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"A car has a door\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'), ('car', 'NN'), ('has', 'VBZ'), ('a', 'DT'), ('door', 'NN')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_text = nltk.pos_tag(text)\n",
    "[('A', 'DT'), ('car', 'NN'), ('has', 'VBZ'), ('a', 'DT'), ('door', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = [pos for (token,pos) in nltk.pos_tag(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  PP -> P NP\n",
    "  NP -> Det N | Det N PP\n",
    "  VP -> V NP | VP PP\n",
    "  Det -> 'DT'\n",
    "  N -> 'NN'\n",
    "  V -> 'VBZ'\n",
    "  P -> 'PP'\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.ChartParser(simple_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parser.parse(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Chart.parses at 0x0F95B530>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'), ('car', 'NN'), ('has', 'VBZ'), ('a', 'DT'), ('door', 'NN')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion1 = nltk.word_tokenize(\"Desperté cubierto de sudor\")\n",
    "oracion2 = nltk.word_tokenize(\"Del piso de ladrillos rojos recién regados subía un vapor caliente\")\n",
    "oracion3 = nltk.word_tokenize(\"Una mariposa de alas grisáceas revoloteaba encandilada alrededor del foco amarillento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> 'saw' | 'ate' | 'walked'\n",
    "  NP -> 'John' | 'Mary' | 'Bob' | 'Desperté'| Det N | Det N PP\n",
    "  Det -> 'a' | 'an' | 'the' | 'my' | 'cubierto'\n",
    "  N -> 'man' | 'dog' | 'cat' | 'telescope' | 'park' | 'de'\n",
    "  P -> 'in' | 'on' | 'by' | 'with'| 'sudor'\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Desperté', 'NNP'), ('cubierto', 'NN'), ('de', 'FW'), ('sudor', 'NN')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag (oracion1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.ChartParser(grammar1)\n",
    "trees = parser.parse_all(oracion1)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Desperté', 'cubierto', 'de', 'sudor']\n"
     ]
    }
   ],
   "source": [
    "print(oracion1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Del', 'NNP'),\n",
       " ('piso', 'NN'),\n",
       " ('de', 'IN'),\n",
       " ('ladrillos', 'FW'),\n",
       " ('rojos', 'JJ'),\n",
       " ('recién', 'NN'),\n",
       " ('regados', 'NN'),\n",
       " ('subía', 'NN'),\n",
       " ('un', 'JJ'),\n",
       " ('vapor', 'NN'),\n",
       " ('caliente', 'NN')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag (oracion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Una', 'NNP'),\n",
       " ('mariposa', 'NN'),\n",
       " ('de', 'FW'),\n",
       " ('alas', 'FW'),\n",
       " ('grisáceas', 'NNS'),\n",
       " ('revoloteaba', 'VBP'),\n",
       " ('encandilada', 'JJ'),\n",
       " ('alrededor', 'NN'),\n",
       " ('del', 'NN'),\n",
       " ('foco', 'NN'),\n",
       " ('amarillento', 'NN')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag (oracion3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
