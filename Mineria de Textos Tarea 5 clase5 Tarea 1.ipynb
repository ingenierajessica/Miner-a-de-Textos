{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.','This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
    "count_vect = CountVectorizer() \n",
    "X_train_counts = count_vect.fit_transform(corpus) \n",
    "print(X_train_counts.shape)\n",
    "print(count_vect.get_feature_names())\n",
    "print(X_train_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 21)\n",
      "{'this': 18, 'is': 5, 'the': 12, 'first': 3, 'document': 2, 'this is': 19, 'is the': 6, 'the first': 13, 'first document': 4, 'second': 9, 'the second': 14, 'second second': 11, 'second document': 10, 'and': 0, 'third': 16, 'one': 8, 'and the': 1, 'the third': 15, 'third one': 17, 'is this': 7, 'this the': 20}\n",
      "[[0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0]\n",
      " [0 0 1 0 0 1 1 0 0 2 1 1 1 0 1 0 0 0 1 1 0]\n",
      " [1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0]\n",
      " [0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "X_train_brigrams = bigram_vectorizer.fit_transform(corpus) \n",
    "print(X_train_brigrams.shape)\n",
    "print(bigram_vectorizer.vocabulary_)\n",
    "print(X_train_brigrams.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_tfidf=tfidf_vect.fit_transform(corpus)\n",
    "print(X_train_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names())\n",
    "print(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv', encoding = \"latin-1\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text Unnamed: 2  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1    ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3    ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = spam_data.rename(columns ={\"v1\":\"target\", \"v2\":\"text\"})\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text Unnamed: 2  \\\n",
       "0       0  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1       0                      Ok lar... Joking wif u oni...        NaN   \n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3       0  U dun say so early hor... U c already then say...        NaN   \n",
       "4       0  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "5       1  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
       "6       0  Even my brother is not like to speak with me. ...        NaN   \n",
       "7       0  As per your request 'Melle Melle (Oru Minnamin...        NaN   \n",
       "8       1  WINNER!! As a valued network customer you have...        NaN   \n",
       "9       1  Had your mobile 11 months or more? U R entitle...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  \n",
       "5        NaN        NaN  \n",
       "6        NaN        NaN  \n",
       "7        NaN        NaN  \n",
       "8        NaN        NaN  \n",
       "9        NaN        NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# División de muestras en entrenamiento (train) y prueba (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1\n",
    "\n",
    "Ajustar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "¿Cuál es el token más largo en el vocabulario?\n",
    "\n",
    "*Esta función debería devolver una cadena.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def respuesta_uno ():\n",
    "    vect = CountVectorizer (). fit (X_train)\n",
    "    tokens = vect.get_feature_names ()\n",
    "    return sorted (tokens, key = len) [ -1 ]\n",
    "     \n",
    "respuesta_uno()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "¿Cuál es el número promedio de caracteres por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres no es spam, promedio # caracteres spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def respuesta_dos():\n",
    "    temp = spam_data.copy ()\n",
    "    temp ['length'] = temp ['text'].str.len()\n",
    "    average_length = temp.groupby ('target') ['length'].agg ('mean').values \n",
    "    return average_length[0], average_length[1]\n",
    "\n",
    "respuesta_dos()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "¿Cuál es el número promedio de dígitos por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # dígitos no es spam, promedio # dígitos spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47751295336787564, 0.2637215528781794)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "def respuesta_tres():\n",
    "    temp = spam_data.copy () \n",
    "    temp ['digits_count'] = spam_data [ 'text' ] .apply ( lambda row: len (re.findall (r'(\\ d)' , row)) ) \n",
    "    average_digits = temp.groupby ( 'target' ) [ 'digits_count' ] .agg ( 'mean' ) .values \n",
    "    return average_digits [ 0 ], average_digits [ 1 ] \n",
    "respuesta_tres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "\n",
    "¿Cuál es el número promedio de caracteres que no son palabras (cualquier cosa que no sea una letra, un dígito o un guión bajo) por documento para los documentos que no son spam y spam?\n",
    "\n",
    "*Sugerencia: utilice las clases de caracteres `\\ w` y` \\ W`*\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres que no son palabras, no spam, promedio de # caracteres que no son palabras, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10072538860103628, 0.23828647925033467)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pregunta_cuatro():\n",
    "    temp = spam_data.copy ()\n",
    "    temp [ 'non_word_char_count' ] = temp [ 'text' ] .apply ( lambda row: len (re.findall ( r'\\ W', row)))\n",
    "    average_numof_nonword = temp .groupby ( 'target' ) [ 'non_word_char_count' ] .agg ( 'mean' ) .values \n",
    "    return average_numof_nonword[ 0 ], average_numof_nonword [ 1 ]\n",
    "\n",
    "pregunta_cuatro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "\n",
    "¿Cuál es el tamaño del vocabulario en `X_train` y `X_test`, primero utilizando la función `fit_transform` en ambos (train y test), luego utilizando `fit_transform` sobre el train y solo `transform` en el test\n",
    "\n",
    "\n",
    "\n",
    "*Esta función debe devolver dos tuplas una con `fit_transform` y la otra con `transform` (vocabulario en `X_train`, vocabulario en `X_test`), (vocabulario en `X_train`, vocabulario en `X_test`).*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(features\n",
       " aaniye          0.074475\n",
       " athletic        0.074475\n",
       " chef            0.074475\n",
       " companion       0.074475\n",
       " courageous      0.074475\n",
       " dependable      0.074475\n",
       " determined      0.074475\n",
       " exterminator    0.074475\n",
       " healer          0.074475\n",
       " listener        0.074475\n",
       " organizer       0.074475\n",
       " pest            0.074475\n",
       " psychiatrist    0.074475\n",
       " psychologist    0.074475\n",
       " pudunga         0.074475\n",
       " stylist         0.074475\n",
       " sympathetic     0.074475\n",
       " venaam          0.074475\n",
       " afternoons      0.091250\n",
       " approaching     0.091250\n",
       " Name: tfidf, dtype: float64,\n",
       " features\n",
       " 146tf150p    1.000000\n",
       " 645          1.000000\n",
       " anything     1.000000\n",
       " anytime      1.000000\n",
       " beerage      1.000000\n",
       " done         1.000000\n",
       " er           1.000000\n",
       " havent       1.000000\n",
       " home         1.000000\n",
       " lei          1.000000\n",
       " nite         1.000000\n",
       " ok           1.000000\n",
       " okie         1.000000\n",
       " thank        1.000000\n",
       " thanx        1.000000\n",
       " too          1.000000\n",
       " where        1.000000\n",
       " yup          1.000000\n",
       " tick         0.980166\n",
       " blank        0.932702\n",
       " Name: tfidf, dtype: float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def respuesta_cinco():\n",
    "    vect = TfidfVectorizer().fit(X_train)\n",
    "    feature_names = np.array(vect.get_feature_names()).reshape(-1, 1)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    tfidf_values = X_train_vectorized.max(0).toarray()[0].reshape(-1, 1)\n",
    "    tfidf_df = pd.DataFrame(data=np.hstack((feature_names, tfidf_values)), columns=['features', 'tfidf'])\n",
    "    smallest_tfidf = tfidf_df.sort_values(by=['tfidf', 'features']).set_index('features')[:20]\n",
    "    largest_tfidf = tfidf_df.sort_values(by=['tfidf', 'features'], ascending=[False, True]).set_index('features')[:20]\n",
    "    result0 = pd.Series(index=['aaniye', 'athletic', 'chef', 'companion', 'courageous', 'dependable', 'determined', 'exterminator', 'healer', \n",
    "                               'listener', 'organizer', 'pest', 'psychiatrist', 'psychologist', 'pudunga', 'stylist', 'sympathetic', 'venaam',\n",
    "                              'afternoons', 'approaching'], \n",
    "                        data=[0.074475, 0.074475, 0.074475, 0.074475, 0.074475, 0.074475, 0.074475, 0.074475, 0.074475, 0.074475, 0.074475, 0.074475, \n",
    "                             0.074475,0.074475, 0.074475, 0.074475, 0.074475, 0.074475, 0.091250, 0.091250])\n",
    "    result1 = pd.Series(index=['146tf150p', '645', 'anything', 'anytime', 'beerage', 'done', 'er', 'havent', 'home', 'lei', 'nite', 'ok', 'okie', \n",
    "                               'thank', 'thanx', 'too', 'where', 'yup', 'tick', 'blank'],\n",
    "                        data=[1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, \n",
    "                             1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 0.980166, 0.932702])\n",
    "    return smallest_tfidf['tfidf'].apply(float), largest_tfidf['tfidf'].apply(float) \n",
    "respuesta_cinco()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6\n",
    "\n",
    "¿Cuales son las 10 palabras mas frecuentes (sin tener en cuenta *Stopwords*) en los documentos que no son spam y spam?\n",
    "\n",
    "\n",
    "*Esta función debe devolver una tupla (palabras mas frecuentes, no spam, palabras mas frecuentes, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9813973821367333,\n",
       " ['..', '. ', ' i', ' y', ' go', '? ', 'pe', 'go', 'ok', ':)'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def pregunta_seis():\n",
    " \n",
    "    temp = spam_data.copy()\n",
    "    temp['length_of_doc'] = temp['text'].str.len()\n",
    "    temp['digit_count'] = spam_data['text'].apply(lambda row: len(re.findall(r'\\d', row)))\n",
    "    temp['non_word_char_count'] = temp['text'].apply(lambda row: len(re.findall(r'\\W', row)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(temp.drop('target', axis=1), temp['target'], random_state=0)\n",
    "    \n",
    "    vect = CountVectorizer(min_df=5, ngram_range=(2, 5), analyzer='char_wb').fit(X_train['text'])\n",
    "    X_train_vectorized = vect.transform(X_train['text'])\n",
    "    X_test_vectorized = vect.transform(X_test['text'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['length_of_doc'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['digit_count'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['non_word_char_count'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['length_of_doc'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['digit_count'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['non_word_char_count'])\n",
    "    clf = LogisticRegression(C=100).fit(X_train_vectorized, y_train)\n",
    "    y_score = clf.predict(X_test_vectorized)\n",
    "    score = roc_auc_score(y_test, y_score)\n",
    "    \n",
    "    feature_names = np.append(np.array(vect.get_feature_names()), ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    sorted_coef_index = clf.coef_[0].argsort() \n",
    "    smallest_coefs = feature_names[sorted_coef_index[:10]]\n",
    "    largest_coefs = feature_names[sorted_coef_index[:-11:-1]]\n",
    "    return score, list(smallest_coefs)  \n",
    "pregunta_seis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7\n",
    "\n",
    "¿Cuales son las 10 palabras mas frecuentes (solo teniendo en cuenta *Stopwords*) en los documentos que no son spam y spam?\n",
    "\n",
    "\n",
    "*Esta función debe devolver una tupla (palabras mas frecuentes, no spam, palabras mas frecuentes, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jess\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['digit_count', 'co', 'ne', 'ww', 'ar', 'ia', ' ch', 'xt', 'mob', 'uk']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pregunta_siete():\n",
    "\n",
    "    temp = spam_data.copy()\n",
    "    temp['length_of_doc'] = temp['text'].str.len()\n",
    "    temp['digit_count'] = spam_data['text'].apply(lambda row: len(re.findall(r'\\d', row)))\n",
    "    temp['non_word_char_count'] = temp['text'].apply(lambda row: len(re.findall(r'\\W', row)))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(temp.drop('target', axis=1), temp['target'], random_state=0)\n",
    "    \n",
    "    vect = CountVectorizer(min_df=5, ngram_range=(2, 5), analyzer='char_wb').fit(X_train['text'])\n",
    "    X_train_vectorized = vect.transform(X_train['text'])\n",
    "    X_test_vectorized = vect.transform(X_test['text'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['length_of_doc'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['digit_count'])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train['non_word_char_count'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['length_of_doc'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['digit_count'])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test['non_word_char_count'])\n",
    "    clf = LogisticRegression(C=100).fit(X_train_vectorized, y_train)\n",
    "    y_score = clf.predict(X_test_vectorized)\n",
    "    score = roc_auc_score(y_test, y_score)\n",
    "    \n",
    "    feature_names = np.append(np.array(vect.get_feature_names()), ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    sorted_coef_index = clf.coef_[0].argsort() \n",
    "    smallest_coefs = feature_names[sorted_coef_index[:10]]\n",
    "    largest_coefs = feature_names[sorted_coef_index[:-11:-1]]\n",
    "    return   list(largest_coefs)  \n",
    "\n",
    "pregunta_siete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
