{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de librerias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "#Tienes que descargarte las stopwords primero via nltk.download()\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict \n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion del archivo que se nos proporciono agregandole cabeceras que no contiene\n",
    "names = ['Id', 'tweet', 'polaridad']\n",
    "df = pd.read_csv('TweetsEtiquetadosPolaridad.txt', sep='\\t',names=names)\n",
    "df2 = pd.read_csv('TweetsEtiquetadosPolaridad.txt', sep='\\t',names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polaridad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>819337926026862593</td>\n",
       "      <td>Antojo de empanada colombiana</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>817464156756574208</td>\n",
       "      <td>Me volvieron a dejar sola</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>819237267181080576</td>\n",
       "      <td>@ManuTonic Buenotes diotas. Un gran abrazo</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id                                       tweet polaridad\n",
       "0  819337926026862593               Antojo de empanada colombiana       NEU\n",
       "1  817464156756574208                   Me volvieron a dejar sola         N\n",
       "2  819237267181080576  @ManuTonic Buenotes diotas. Un gran abrazo         P"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compre', 'Hasta', 'Borinquen', 'internet', 'ver', 'si', 'aprende', 'poquito', 'dice', 'bobadas']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paso - 1c: Tokenización: en este cada entrada en el corpus se dividirá en un conjunto de palabras\n",
    "df['tweet']= [word_tokenize(entry) for entry in df['tweet']]\n",
    "\n",
    "# Paso - 1d: Eliminar palabras de detención, no numéricas y realizar el vástago / lemmenting de palabras.\n",
    "# WordNetLemmatizer requiere etiquetas Pos para comprender si la palabra es sustantivo o verbo o adjetivo, etc. \n",
    "# De manera predeterminada, está configurada como Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(df['tweet']):\n",
    "     # Declarar la lista vacía para almacenar las palabras que siguen las reglas de este paso\n",
    "    Final_words = []\n",
    "    # Inicializando WordNetLemmatizer ()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # La función pos_tag a continuación proporcionará la 'etiqueta', es decir, si la palabra es Sustantivo (N) o Verbo (V) u otra cosa.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # La siguiente condición es verificar las palabras Stop y considerar solo alfabetos\n",
    "        if word not in stopwords.words('spanish') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # El conjunto final procesado de palabras para cada iteración se almacenará en 'text_final'\n",
    "    df.loc[index,'tweet_final'] = str(Final_words)\n",
    "print(df.loc[index,'tweet_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polaridad</th>\n",
       "      <th>tweet_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>819337926026862593</td>\n",
       "      <td>[Antojo, de, empanada, colombiana]</td>\n",
       "      <td>NEU</td>\n",
       "      <td>['Antojo', 'empanada', 'colombiana']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>817464156756574208</td>\n",
       "      <td>[Me, volvieron, a, dejar, sola]</td>\n",
       "      <td>N</td>\n",
       "      <td>['Me', 'volvieron', 'dejar', 'sola']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>819237267181080576</td>\n",
       "      <td>[@, ManuTonic, Buenotes, diotas, ., Un, gran, ...</td>\n",
       "      <td>P</td>\n",
       "      <td>['ManuTonic', 'Buenotes', 'diotas', 'Un', 'gra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id                                              tweet  \\\n",
       "0  819337926026862593                 [Antojo, de, empanada, colombiana]   \n",
       "1  817464156756574208                    [Me, volvieron, a, dejar, sola]   \n",
       "2  819237267181080576  [@, ManuTonic, Buenotes, diotas, ., Un, gran, ...   \n",
       "\n",
       "  polaridad                                        tweet_final  \n",
       "0       NEU               ['Antojo', 'empanada', 'colombiana']  \n",
       "1         N               ['Me', 'volvieron', 'dejar', 'sola']  \n",
       "2         P  ['ManuTonic', 'Buenotes', 'diotas', 'Un', 'gra...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[', \"'\")\n",
      "(\"'\", 'A')\n",
      "('A', 'n')\n",
      "('n', 't')\n",
      "('t', 'o')\n",
      "('o', 'j')\n",
      "('j', 'o')\n",
      "('o', \"'\")\n",
      "(\"'\", ',')\n",
      "(',', ' ')\n",
      "(' ', \"'\")\n",
      "(\"'\", 'e')\n",
      "('e', 'm')\n",
      "('m', 'p')\n",
      "('p', 'a')\n",
      "('a', 'n')\n",
      "('n', 'a')\n",
      "('a', 'd')\n",
      "('d', 'a')\n",
      "('a', \"'\")\n",
      "(\"'\", ',')\n",
      "(',', ' ')\n",
      "(' ', \"'\")\n",
      "(\"'\", 'c')\n",
      "('c', 'o')\n",
      "('o', 'l')\n",
      "('l', 'o')\n",
      "('o', 'm')\n",
      "('m', 'b')\n",
      "('b', 'i')\n",
      "('i', 'a')\n",
      "('a', 'n')\n",
      "('n', 'a')\n",
      "('a', \"'\")\n",
      "(\"'\", ']')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams \n",
    "n = 2\n",
    "twograms = ngrams(df.loc[index,'tweet_final'], n)\n",
    "for grams in twograms:\n",
    "     print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[', \"'\", 'A')\n",
      "(\"'\", 'A', 'n')\n",
      "('A', 'n', 't')\n",
      "('n', 't', 'o')\n",
      "('t', 'o', 'j')\n",
      "('o', 'j', 'o')\n",
      "('j', 'o', \"'\")\n",
      "('o', \"'\", ',')\n",
      "(\"'\", ',', ' ')\n",
      "(',', ' ', \"'\")\n",
      "(' ', \"'\", 'e')\n",
      "(\"'\", 'e', 'm')\n",
      "('e', 'm', 'p')\n",
      "('m', 'p', 'a')\n",
      "('p', 'a', 'n')\n",
      "('a', 'n', 'a')\n",
      "('n', 'a', 'd')\n",
      "('a', 'd', 'a')\n",
      "('d', 'a', \"'\")\n",
      "('a', \"'\", ',')\n",
      "(\"'\", ',', ' ')\n",
      "(',', ' ', \"'\")\n",
      "(' ', \"'\", 'c')\n",
      "(\"'\", 'c', 'o')\n",
      "('c', 'o', 'l')\n",
      "('o', 'l', 'o')\n",
      "('l', 'o', 'm')\n",
      "('o', 'm', 'b')\n",
      "('m', 'b', 'i')\n",
      "('b', 'i', 'a')\n",
      "('i', 'a', 'n')\n",
      "('a', 'n', 'a')\n",
      "('n', 'a', \"'\")\n",
      "('a', \"'\", ']')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams \n",
    "n = 3\n",
    "threegrams = ngrams(df.loc[index,'tweet_final'], n)\n",
    "for grams in threegrams:\n",
    "     print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[', \"'\", 'A', 'n')\n",
      "(\"'\", 'A', 'n', 't')\n",
      "('A', 'n', 't', 'o')\n",
      "('n', 't', 'o', 'j')\n",
      "('t', 'o', 'j', 'o')\n",
      "('o', 'j', 'o', \"'\")\n",
      "('j', 'o', \"'\", ',')\n",
      "('o', \"'\", ',', ' ')\n",
      "(\"'\", ',', ' ', \"'\")\n",
      "(',', ' ', \"'\", 'e')\n",
      "(' ', \"'\", 'e', 'm')\n",
      "(\"'\", 'e', 'm', 'p')\n",
      "('e', 'm', 'p', 'a')\n",
      "('m', 'p', 'a', 'n')\n",
      "('p', 'a', 'n', 'a')\n",
      "('a', 'n', 'a', 'd')\n",
      "('n', 'a', 'd', 'a')\n",
      "('a', 'd', 'a', \"'\")\n",
      "('d', 'a', \"'\", ',')\n",
      "('a', \"'\", ',', ' ')\n",
      "(\"'\", ',', ' ', \"'\")\n",
      "(',', ' ', \"'\", 'c')\n",
      "(' ', \"'\", 'c', 'o')\n",
      "(\"'\", 'c', 'o', 'l')\n",
      "('c', 'o', 'l', 'o')\n",
      "('o', 'l', 'o', 'm')\n",
      "('l', 'o', 'm', 'b')\n",
      "('o', 'm', 'b', 'i')\n",
      "('m', 'b', 'i', 'a')\n",
      "('b', 'i', 'a', 'n')\n",
      "('i', 'a', 'n', 'a')\n",
      "('a', 'n', 'a', \"'\")\n",
      "('n', 'a', \"'\", ']')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams \n",
    "n = 4\n",
    "fourgrams = ngrams(df.loc[index,'tweet_final'], n)\n",
    "for grams in fourgrams:\n",
    "     print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[', \"'\")\n",
      "(\"'\", 'A')\n",
      "('A', 'n')\n",
      "('n', 't')\n",
      "('t', 'o')\n",
      "('o', 'j')\n",
      "('j', 'o')\n",
      "('o', \"'\")\n",
      "(\"'\", ',')\n",
      "(',', ' ')\n",
      "(' ', \"'\")\n",
      "(\"'\", 'e')\n",
      "('e', 'm')\n",
      "('m', 'p')\n",
      "('p', 'a')\n",
      "('a', 'n')\n",
      "('n', 'a')\n",
      "('a', 'd')\n",
      "('d', 'a')\n",
      "('a', \"'\")\n",
      "(\"'\", ',')\n",
      "(',', ' ')\n",
      "(' ', \"'\")\n",
      "(\"'\", 'c')\n",
      "('c', 'o')\n",
      "('o', 'l')\n",
      "('l', 'o')\n",
      "('o', 'm')\n",
      "('m', 'b')\n",
      "('b', 'i')\n",
      "('i', 'a')\n",
      "('a', 'n')\n",
      "('n', 'a')\n",
      "('a', \"'\")\n",
      "(\"'\", ']')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams \n",
    "n = 2\n",
    "twograms = nltk.ngrams(df.loc[index,'tweet_final'], n)\n",
    "for grams in twograms:\n",
    "     print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df2.loc[index,'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Por dos palabras==================\n",
      "[('Antojo', 'de'), ('de', 'empanada'), ('empanada', 'colombiana')]\n",
      "\n",
      "Por tres palabras==================\n",
      "[('Antojo', 'de', 'empanada'), ('de', 'empanada', 'colombiana')]\n",
      "\n",
      "Por cuatro palabras==================\n",
      "[('Antojo', 'de', 'empanada', 'colombiana')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams \n",
    "n = 4\n",
    "nltk_tokens = nltk.word_tokenize(text) \n",
    "print(\"\\nPor dos palabras==================\")\n",
    "print(list(nltk.ngrams(nltk_tokens,2)))\n",
    "print(\"\\nPor tres palabras==================\")\n",
    "print(list(nltk.ngrams(nltk_tokens,3)))\n",
    "print(\"\\nPor cuatro palabras==================\")\n",
    "print(list(nltk.ngrams(nltk_tokens,4)))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
